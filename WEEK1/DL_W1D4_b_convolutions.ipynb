{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_arhPUkn4yYA"
   },
   "source": [
    "#**Week 1 day 4: Introduction to image** **processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApPYRq2H49yv"
   },
   "source": [
    "## Exercise 3:  Implement your own version of the cross-correlation operator\n",
    "\n",
    "(exercise in the slides, book [7.2.1](http://d2l.ai/chapter_convolutional-neural-networks/conv-layer.html#the-cross-correlation-operation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3-KFbk4vTKR0",
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "pVD8w85ScE1o",
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor I:\n",
      " tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]], dtype=torch.int32)\n",
      "\n",
      "Convolutional filter W1:\n",
      " tensor([[0., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "# Fill in the gaps!\n",
    "I = torch.IntTensor(range(1, 10, 1)).reshape(-1, 3)\n",
    "\n",
    "W1 = torch.FloatTensor([0, 2, 3 , 4]).reshape(-1,2)\n",
    "\n",
    "\n",
    "#W2 = torch.FloatTensor(256)\n",
    "\n",
    "print(\"Input tensor I:\\n\", I)\n",
    "print(\"\\nConvolutional filter W1:\\n\", W1)\n",
    "#print(\"\\nConvolutional filter W2: \\n\", W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 10.        , 190.22222222],\n",
       "       [ 27.625     , 204.22222222]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.empty((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  4.],\n",
       "        [ 9., 16.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1 * W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [4, 5]], dtype=torch.int32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I[:2, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'torch.Size' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mW1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'torch.Size' object is not callable"
     ]
    }
   ],
   "source": [
    "W1.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mI\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "I[0.0:2, 0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "0yGdtoNDVIX1",
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[36., 45.],\n",
       "       [63., 72.]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your own function for cross-correlations\n",
    "# function for cross-correlations\n",
    "def corr2d(I,f):\n",
    "\n",
    "    new_mat = np.empty((2,2))\n",
    "    \n",
    "    ih, iw = I.shape[0], I.shape[1]\n",
    "    fh, fw = f.shape[0], f.shape[1]\n",
    "\n",
    "    for h in range(new_mat.shape[0]):\n",
    "        for w in range(new_mat.shape[1]):\n",
    "            new_mat[h,w] = (I[h: h+fh, w:w+fw] * f).sum()\n",
    "            \n",
    "    \n",
    "            \n",
    "            \n",
    "    \n",
    "    return new_mat\n",
    "corr2d(I,W1)\n",
    "#print('W1:', corr2d(I,W1))\n",
    "#print('W2 (ones):', corr2d(I,W2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "vD4khKK0o6ME",
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sample_data/mnist_train_small.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Try out your function on the example image from MNIST\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m mnist_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msample_data/mnist_train_small.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m mnist_image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(mnist_df\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m:]))\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m28\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Create figure with original, and result from filter W1 and W2\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/pytorch-env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/pytorch-env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/pytorch-env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/pytorch-env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/pytorch-env/lib/python3.10/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sample_data/mnist_train_small.csv'"
     ]
    }
   ],
   "source": [
    "# Try out your function on the example image from MNIST\n",
    "mnist_df = pd.read_csv(\"sample_data/mnist_train_small.csv\", header=None)\n",
    "mnist_image = torch.tensor(np.array(mnist_df.loc[1,1:])).resize_(28,28)\n",
    "\n",
    "# Create figure with original, and result from filter W1 and W2\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "fig.add_subplot(1, 3, 1)\n",
    "plt.imshow(mnist_image,cmap='gray')\n",
    "plt.title(\"Original image\")\n",
    "\n",
    "fig.add_subplot(1, 3, 2)\n",
    "plt.imshow(corr2d(mnist_image, W1),cmap='gray')\n",
    "plt.title(\"W1 applied\")\n",
    "\n",
    "fig.add_subplot(1, 3, 3)\n",
    "plt.imshow(corr2d(mnist_image, W2),cmap='gray')\n",
    "plt.title(\"W2 applied\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptHUg6_R8Hka"
   },
   "source": [
    "### Exercise 3b: Convolutions in PyTorch\n",
    "Now lets repeat the above exercise, using the built-in *Conv2d* class of Pytorch\n",
    "- [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) works the same as the corr2d function, except it expects a 4D tensor of dimensions: (batch, channel, height, width)\n",
    "- As our data is currently a 2D tensor of dimensions (height, width), we need to add two dimensions of length 1 with the [torch.reshape()](https://pytorch.org/docs/stable/generated/torch.reshape.html) function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jp4kaptZ8Hka",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Implement with `nn.Conv2d`\n",
    "# Define the image and weights, as 4-d tensors\n",
    "I = torch.tensor([[3,1,4,3],\n",
    "                  [3,2,3,1],\n",
    "                  [4,3,6,4],\n",
    "                  [3,3,1,7]], dtype = torch.float).reshape(1,1,4,4)\n",
    "\n",
    "# Same for W1 - note the alternative syntax\n",
    "W1 = W1.float().reshape(1,1,3,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E2eXMxgh8Hka",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# define the filter\n",
    "k1 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(3,3), bias=False)\n",
    "# assign weights. Note requires to use the Parameter class for proper initialization\n",
    "# of the autograder. Not useful for this exercise but essential for deep learning :)\n",
    "k1.weight = nn.Parameter(W1)\n",
    "# apply to image I\n",
    "k1(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZS2eTwTqb5F4"
   },
   "source": [
    "**Exercise** Create the convolutional filter for W2, and apply it on I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cg_8GHwwcCSM",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2bJM70P613F"
   },
   "source": [
    "## Exercise 4: Experimenting with filters\n",
    "The website below contains an interactive tool, where you try out different 3x3 convolutional filters. Experiment with them and try out different custom filters.\n",
    "\n",
    "https://beej.us/blog/data/convolution-image-processing/\n",
    "\n",
    "**Question 1**: Can you design a custom filter that highlights only vertical edges?\n",
    "\n",
    "**Question 2**: Can you create a filter that rotates the entire image 90 degrees? Why or why not?\n",
    "\n",
    "### Exercise 4b - implement filters with corr2d and Conv2d\n",
    "\n",
    "Below some more filters are given, that you can apply to the example image from mnist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YhoSykSuqCci",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Get convolution matrices from here:\n",
    "# https://en.wikipedia.org/wiki/Kernel_(image_processing)\n",
    "# https://beej.us/blog/data/convolution-image-processing/\n",
    "\n",
    "edge = torch.tensor([[-1,-1,-1],\n",
    "                     [-1, 8,-1],\n",
    "                     [-1,-1,-1]])\n",
    "sharpen = torch.tensor([[0 ,-1,0],\n",
    "                        [-1, 16,-1],\n",
    "                        [0,-1,0]])\n",
    "emboss = torch.tensor([[-2, -1, 0],\n",
    "                       [-1,  1, 1],\n",
    "                       [ 0, 1, 2]])\n",
    "\n",
    "\n",
    "# Create figure with original, and result from filter W1 and W2\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "fig.add_subplot(1, 4, 1)\n",
    "plt.imshow(mnist_image,cmap='gray')\n",
    "plt.title(\"Original image\")\n",
    "\n",
    "fig.add_subplot(1, 4, 2)\n",
    "plt.imshow(corr2d(mnist_image, edge),cmap='gray')\n",
    "plt.title(\"Edge applied\")\n",
    "\n",
    "fig.add_subplot(1, 4, 3)\n",
    "plt.imshow(corr2d(mnist_image, sharpen),cmap='gray')\n",
    "plt.title(\"Sharpen applied\")\n",
    "\n",
    "fig.add_subplot(1, 4, 4)\n",
    "plt.imshow(corr2d(mnist_image, emboss),cmap='gray')\n",
    "plt.title(\"Emboss applied\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liDqMbwfdel3"
   },
   "source": [
    "**Exercise** Use the Conv2d class of pytorch to create an edge-detecting filter, and apply in on the MNIST sample image.\n",
    "\n",
    "Remember to change the dimentions of the image to what Conv2d expects (Batch, Channels, Height, Width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ikRoif3Zddli",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Fill in the gaps\n",
    "# define the filter\n",
    "filter = \n",
    "# assign weights. Note requires to use the Parameter class for proper initialization\n",
    "# of the autograder. Not useful for this exercise but essential for deep learning :)\n",
    "filter.weight = \n",
    "\n",
    "\n",
    "modified = filter(mnist_image.float().reshape(1,1,28,28))\n",
    "print(modified[0,0].shape)\n",
    "plt.imshow(modified[0].detach().permute(1,2,0), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IajHtE84NMLE"
   },
   "source": [
    "\n",
    "## Exercise 5: Cross correlation operator for multiple channels\n",
    "(exercise in the slides, book [7.4.1](http://d2l.ai/chapter_convolutional-neural-networks/channels.html#multiple-input-channels))\n",
    "\n",
    "Using Conv2d, implement the following cross-correlation computation with 2 input channels\n",
    "\n",
    "![](http://d2l.ai/_images/conv-multi-in.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qD804fVANLv-",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Write the input tensor (both channels)\n",
    "# What shape should it be?\n",
    "# Recall that nn.Conv2d works with 4 dimentional tensors: batch, channel, height, width\n",
    "\n",
    "input = torch.tensor(3,)\n",
    "\n",
    "# write the kernel tensor (it has 2 filters, one per band)\n",
    "kernel = \n",
    "\n",
    "# define the filter using Conv2d\n",
    "f = nn.Conv2d\n",
    "\n",
    "# assign the filter weights\n",
    "\n",
    "# apply the filter on the input\n",
    "f(input) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Z8jvx_U8Hkb"
   },
   "source": [
    "Of course in our networks, we will not be assigning the kernel weights. We will learn them from data!\n",
    "\n",
    "How? Check the following exercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUGtflHRLvBN"
   },
   "source": [
    "## Exercise 6: Learn kernel weights from data!\n",
    "Exercise in the slides, book [7.2.3](http://d2l.ai/chapter_convolutional-neural-networks/conv-layer.html#object-edge-detection-in-images), [7.2.4](http://d2l.ai/chapter_convolutional-neural-networks/conv-layer.html#learning-a-kernel)\n",
    "\n",
    "**Data preparation**\n",
    "\n",
    "First step: Construct an image as a tensor X, sized 6Ã—8.\n",
    "\n",
    "The middle four columns are black (0) and the rest are white (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z39rAAsGLtU1",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "X = torch.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "print(X)\n",
    "plt.imshow(X, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQiC0Wiijf36"
   },
   "source": [
    "Next, we construct a kernel K with a height of 1 and a width of 2. When we perform the cross-correlation operation with the input, if the horizontally adjacent elements are the same, the output is 0. Otherwise, the output is non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kobabfjWjfUz",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "K = torch.tensor([[1,-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pn-J7KnmjxPG"
   },
   "source": [
    "We are ready to perform the cross-correlation operation with arguments X (our input) and K (our kernel).\n",
    "\n",
    "As you can see, we detect 1 for the edge from white to black and -1 for the edge from black to white. All other outputs take value 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0CUjk9FjwgT",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# use your own corr2d or the one from the d2l package\n",
    "Y = corr2d(X, K)\n",
    "print(Y)\n",
    "plt.imshow(Y, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FM1ZmM2kk0VG"
   },
   "source": [
    "**Learning a Kernel**\n",
    "\n",
    "Designing an edge detector by finite differences `[1, -1]` is neat\n",
    "if we know this is precisely what we are looking for.\n",
    "However, as we look at larger kernels,\n",
    "and consider successive layers of convolutions,\n",
    "it might be impossible to specify\n",
    "precisely what each filter should be doing manually.\n",
    "\n",
    "Now let us see whether we can learn the kernel that generated `Y` from `X`\n",
    "by looking at the input--output pairs only.\n",
    "\n",
    "We first construct a convolutional layer\n",
    "and initialize its kernel as a random tensor.\n",
    "Next, in each iteration, we will use the squared error\n",
    "to compare `Y` with the output of the convolutional layer.\n",
    "We can then calculate the gradient to update the kernel.\n",
    "For the sake of simplicity,\n",
    "in the following\n",
    "we use the built-in class\n",
    "for two-dimensional convolutional layers\n",
    "and ignore the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhY3_sq0k9Bb",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Construct a two-dimensional convolutional layer with 1 output channel and a\n",
    "# kernel of shape (1, 2). For the sake of simplicity, we ignore the bias here\n",
    "conv2d = nn.Conv2d(1,1, kernel_size=(1, 2), bias=False)\n",
    "\n",
    "# The two-dimensional convolutional layer uses four-dimensional input and\n",
    "# output in the format of (batch, channel, height, width).\n",
    "# In our case where the batch size (number of examples in the batch) and\n",
    "# the number of channels are both 1\n",
    "X = X.reshape((1, 1, 6, 8))\n",
    "Y = Y.reshape((1, 1, 6, 7))\n",
    "\n",
    "for i in range(10):\n",
    "    Y_hat = conv2d(X)                 # Apply the kernel\n",
    "    l = (Y_hat - Y) ** 2              # Calculate the loss\n",
    "    conv2d.zero_grad()                # Set the existing gradients to zero\n",
    "    l.sum().backward()                # Backpropagate the losses\n",
    "\n",
    "    # Update the kernel weights with the new gradients\n",
    "    conv2d.weight.data[:] -= 3e-2 * conv2d.weight.grad\n",
    "    if (i + 1) % 2 == 0:\n",
    "        print(f'batch {i + 1}, loss {l.sum():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AeERnsTfl8e3"
   },
   "source": [
    "Note that the error has dropped to a small value after 10 iterations. Now we will take a look at the kernel tensor we learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F1CemOrGl92Z",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "conv2d.weight.data.reshape((1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXMCs-6mmCie"
   },
   "source": [
    "Indeed, the learned kernel tensor is remarkably close\n",
    "to the kernel tensor `K` we defined earlier. Now let's apply it to our original data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cP_f-OwQGXSB",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "Y_hat = conv2d(X).detach()[0][0]\n",
    "\n",
    "print(Y_hat)\n",
    "plt.imshow(Y_hat, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjOGwUdWG2RM"
   },
   "source": [
    "You've made it! That was the first convolutional filter you have learned from data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "heieuDI9j90J"
   },
   "source": [
    "**Exercise** If you have time: learn the edge detection filter using one MNIST image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bKKMqTxHj7W3",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "X = mnist_image.float()/256\n",
    "Y = corr2d(mnist_image, edge)\n",
    "\n",
    "# Necessary reshapes for Conv2d\n",
    "X = X.reshape((1, 1, 28, 28))\n",
    "Y = Y.reshape((1, 1, 26, 26))\n",
    "\n",
    "# Construct a two-dimensional convolutional layer with 1 output channel and a\n",
    "# kernel of shape (3,3). For the sake of simplicity, we ignore the bias here\n",
    "conv2d = \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C4QJ5RWWpzXp",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(conv2d(X).detach()[0][0], cmap='gray')\n",
    "plt.title(\"Estimated image\")\n",
    "\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.imshow(Y[0][0], cmap='gray')\n",
    "plt.title(\"Original image\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bp14aee5krVW",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(conv2d.weight.data[0][0])\n",
    "plt.title(\"Estimated filter\")\n",
    "\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.imshow(edge)\n",
    "plt.title(\"Original filter\")\n",
    "fig.show()\n",
    "\n",
    "plt.imshow(edge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFIggMcBktzk",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "conv2d.weight.data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qoyDqkiPrxbl",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:pytorch-env]",
   "language": "python",
   "name": "conda-env-pytorch-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
