{"cells":[{"cell_type":"markdown","metadata":{"id":"QF4NHGkRDpyQ"},"source":["The following additional libraries are needed to run this\n","notebook. Note that running on Colab is experimental, please report a Github\n","issue if you have any problem."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fsgulVHcDpyV"},"outputs":[],"source":["!pip install d2l"]},{"cell_type":"markdown","metadata":{"id":"6TRmqIcYDpyV","origin_pos":0},"source":["# Concise Implementation of Multilayer Perceptrons\n",":label:`sec_mlp_concise`\n","\n","As you might expect, by relying on the high-level APIs,\n","we can implement MLPs even more concisely.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4C_mLrjfDpyV","origin_pos":2,"tab":["pytorch"]},"outputs":[],"source":["from d2l import torch as d2l\n","import torch\n","from torch import nn"]},{"cell_type":"markdown","metadata":{},"source":["## Model\n","\n","<center><img src=\"https://git.wur.nl/koots006/wias-course-image-and-video-analysis/-/raw/main/day3/classification_network_784_256_10.png\" width=400></center>\n","\n","We again are going to build the network depicted above.\n","\n","* Using `nn.Sequential()`, we can put together the different layers and activation functions that we want to use to create the network\n","* `nn.Flatten()` changes the 2D image to a 1D vector input\n","* `nn.Linear(nr_in, nr_out)` creates a layer with `nr_out` neurons, getting input from `nr_in` elements in the previous layer.\n","* `nn.ReLU()` applies the ReLU activation function to the output of the previsou layer.\n","\n","To initialize the weights with small random numbers, we apply the function `init_weights`, which draws random numbers from a normal distribution with a standard deviation of 0.01.\n","\n","**Exercise:**\n","* Study the code below\n","* Run the code to define and initialize the network\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XvdlBkPaDpyW","origin_pos":4},"source":["## Model\n","\n","As compared with our concise implementation\n","of softmax regression implementation\n","(:numref:`sec_softmax_concise`),\n","the only difference is that we add\n","*two* fully-connected layers\n","(previously, we added *one*).\n","The first is our hidden layer,\n","which contains 256 hidden units\n","and applies the ReLU activation function.\n","The second is our output layer.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iqC__AnuDpyW","origin_pos":6,"tab":["pytorch"]},"outputs":[],"source":["net = nn.Sequential(nn.Flatten(),\n","                    nn.Linear(784, 256),\n","                    nn.ReLU(),\n","                    nn.Linear(256, 10))\n","\n","def init_weights(m):\n","    if type(m) == nn.Linear:\n","        nn.init.normal_(m.weight, std=0.01)\n","\n","net.apply(init_weights);"]},{"cell_type":"markdown","metadata":{"id":"OsUw2AHZDpyX","origin_pos":8},"source":["The training loop is exactly the same\n","as when we implemented softmax regression.\n","This modularity enables us to separate\n","matters concerning the model architecture\n","from orthogonal considerations.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i8njba7uDpyX","origin_pos":10,"tab":["pytorch"]},"outputs":[],"source":["batch_size, lr, num_epochs = 256, 0.1, 10\n","loss = nn.CrossEntropyLoss()\n","trainer = torch.optim.SGD(net.parameters(), lr=lr)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3C1mfd7kDpyX","origin_pos":12,"tab":["pytorch"]},"outputs":[],"source":["train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n","d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SYA4IVWlOZsP"},"outputs":[],"source":["acc_test = d2l.evaluate_accuracy(net, test_iter)\n","print(\"The accuracy on the test set is: \", acc_test)"]},{"cell_type":"markdown","metadata":{"id":"1k5k8aIhDpyY","origin_pos":13},"source":["## Summary\n","\n","* Using high-level APIs, we can implement MLPs much more concisely.\n","* For the same classification problem, the implementation of an MLP is the same as that of softmax regression except for additional hidden layers with activation functions.\n","\n","## Exercises\n","\n","We will experiment with a number of hyperparameters (network architecture, learning rate, batch size)\n","\n","\n","1. Try adding different 1 or 2 extra hidden layers with 256 neurons per layer. What setting (keeping other parameters and hyperparameters constant) works best? <br>\n","NB. If you have two hidden layers with both 256 neurons, think about the number of weights you need between these two layers.\n","\n","2. How does changing the learning rate alter your results? Fixing the model architecture and other hyperparameters (including number of epochs), what learning rate gives you the best results? (Note: try with a learning rate of 1 and 0.0001)\n","\n","3. Try out different activation functions (relu, sigmoids, tanh). Which ones work best?\n","You see that the results might change depending on the hyperparameter setting. This is an important aspect of working with neural networks. Obviously, we want to deal with all hyperparameters at the same time.\n","\n","4. Describe why it is much more challenging to deal with multiple hyperparameters. \n","\n","5. Think of a strategy to optimize over all the parameters (learning rate, iterations, number of hidden layers, number of hidden units per layer) jointly? "]},{"cell_type":"markdown","metadata":{"id":"h61NoaWbYAPR"},"source":["## The XOR problem\n","The XOR or Exclusive OR problem has been an historical problem in the Neural Networks field. It can be defined as follows: given 2 inputs `x1` and `x2`, the output `y` will be true only when just one of the inputs is true. The following table exemplifies it:\n","\n","| x1  | x2  | y   |\n","| --- | --- | --- |\n","| 0   | 0   | 0   |\n","| 0   | 1   | 1   |\n","| 1   | 0   | 1   |\n","| 1   | 1   | 0   | \n","\n","The next block creates a Pytorch dataset class which generates a XOR data distribution.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xrUHkHUdeXt2"},"outputs":[],"source":["from torch.utils.data import Dataset\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# A dataset class for the tomato spectra dataset\n","class XORDataset(Dataset):\n","    \"\"\"\n","    Dataset class used to create a XOR distribution.\n","    It inherits the torch.utils.data.Dataset class. For more info about writing custom datasets classes check this [post](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)\n","\n","    Parameters\n","    ----------\n","    size : size of the dataset\n","    noise : represent the random noise on the data, the larger it is the more noise the data will have\n","    \"\"\"\n","\n","    def __init__(self, size=50, noise=0.5):\n","        x1 = np.array([0., 0., 1., 1.], dtype = np.float32)\n","        x2 = np.array([0., 1., 0., 1.], dtype = np.float32)\n","        y  = np.array([0, 1, 1, 0], dtype = np.int32)\n","\n","        x1 = np.repeat(x1, size)\n","        x2 = np.repeat(x2, size)\n","        self.y =  np.repeat(y,  size)\n","\n","        # Add noise to data points just to have some data variety\n","        self.x1 = x1 + np.random.rand(x1.shape[0])*noise\n","        self.x2 = x2 + np.random.rand(x2.shape[0])*noise\n","\n","        # Shuffle\n","        index_shuffle = np.arange(self.x1.shape[0])\n","        np.random.shuffle(index_shuffle)\n","\n","        self.x1 = self.x1[index_shuffle]\n","        self.x2 = self.x2[index_shuffle]\n","        self.y  = self.y [index_shuffle]\n","\n","    def __len__(self):\n","        return len(self.y)\n","\n","    def __getitem__(self, idx):\n","        data = torch.tensor([self.x1[idx], self.x2[idx]], dtype=torch.float32)\n","        label = torch.tensor(self.y[idx], dtype=torch.int64)\n","        return data, label\n","\n","    def get_all(self):\n","        return self.x1, self.x2, self.y"]},{"cell_type":"markdown","metadata":{"id":"BE5dpqfYZ6P5"},"source":["Let's create a random XOR distribution using the `XORDataset` class and see how it looks like."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fgYGTaUDeYkS"},"outputs":[],"source":["dataset = XORDataset(noise=0.8)\n","x1, x2, y = dataset.get_all()\n","plt.scatter(x1, x2, c=y, cmap=plt.cm.coolwarm)"]},{"cell_type":"markdown","metadata":{"id":"cnJtTHcxyerd"},"source":["**Exercise 6:** What happens in the graph if you set `noise` to 0?\n","\n","Run the next code block to get a function to train a model for this XOR data and to plot the results:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FFy4TcEhkrQW"},"outputs":[],"source":["# Let's adapt a previously defined training function (we change ylim in animator)\n","def train_XOR(net, train_iter, test_iter, loss, num_epochs, updater):\n","    \"\"\"Train a model (similar to the one defined in Chapter 3).\"\"\"\n","    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],\n","                        legend=['train loss', 'train acc', 'test acc'])\n","    for epoch in range(num_epochs):\n","        train_metrics = d2l.train_epoch_ch3(net, train_iter, loss, updater)\n","        test_acc = d2l.evaluate_accuracy(net, test_iter)\n","        animator.add(epoch + 1, train_metrics + (test_acc,))\n","    train_loss, train_acc = train_metrics\n","\n","\n","# Let's create a function to plot the decision boundary of a model\n","def plot_with_decision_boundary(model, X, y):\n","    model.eval()\n","    h = .01  # step size in the mesh\n","\n","    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n","    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n","                        np.arange(y_min, y_max, h))\n","    \n","    Z = []\n","    for i, j in zip(xx.ravel(), yy.ravel()):\n","        x_ = torch.tensor([i, j], dtype=torch.float32)\n","        y_hat = net(x_)\n","        Z.append(y_hat.argmax().detach().cpu().numpy())\n","    Z = np.array(Z)\n","    Z = np.rint(Z)\n","\n","    # Put the result into a color plot\n","    Z = Z.reshape(xx.shape)\n","    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.7)\n","    plt.axis('off')\n","\n","    # Plot also the training points\n","    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)"]},{"cell_type":"markdown","metadata":{"id":"oSS7g0nIaXzG"},"source":["\n","\n","**Exercise 7:** \n","\n","Use and complete the following code (marked with `TODO`) to create, train and observe the results. Note\n","* Create a training and testset using `XORDataset(size, noise=0.8)` where you set `size` to 80 for the training set and to 20 for the test set.\n","* Define the architecture of our network. Note that your network should have two output nodes, since we have two classes\n","* Make sure to randomly initialize the weights in the network using a normal distribution with standard deviation of 0.02\n","\n","\n","Test different network architectures:\n","1. A perceptron\n","2. A MLP with 1 hidden layer and 16 hidden neurons. Use ReLU activation after the hidden layer.\n","3. Remove the ReLU activation, can the problem be solved now?\n","4. Initiate the weights to a constant value instead of a random value. You can use the function `nn.init.constant_` (see [documentation](https://pytorch.org/docs/stable/nn.init.html). What happens now?\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"48KXhcXnlhBl"},"outputs":[],"source":["# Defining some parameters\n","batch_size, lr, num_epochs = 16, 0.006, 100\n","\n","# Creating dataset and dataloaders\n","### TODO: ADD YOUR CODE HERE (~2 lines), using the function XORDataset(size, noise)\n","train_dataset = ..\n","test_dataset = ..\n","\n","train_loader = torch.utils.data.DataLoader(\n","    train_dataset, batch_size=batch_size, shuffle=True,\n","    num_workers=2)\n","\n","test_loader = torch.utils.data.DataLoader(\n","    test_dataset, batch_size=batch_size, shuffle=False,\n","    num_workers=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AifwkTtok5t6"},"outputs":[],"source":["# Let's define the architecture of our network.\n","# Note that your network should have two output node, since we have two classes\n","\n","### TODO: ADD YOUR CODE HERE (~1 line) to define a network\n","net = ..\n","\n","# Init weights\n","def init_weights(m):\n","    if type(m) == nn.Linear:\n","        ### TODO: ADD YOUR CODE HERE (~1 line)\n","\n","net.apply(init_weights)\n","trainer = torch.optim.Adam(net.parameters(), lr=lr)\n","loss = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BXSeCo4glALb"},"outputs":[],"source":["# Train\n","train_XOR(net, train_loader, test_loader, loss, num_epochs, trainer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J8PYk20SStVX"},"outputs":[],"source":["# Plot results in test set with decision boundary\n","x1, x2, y = test_dataset.get_all()\n","X = np.vstack((x1,x2)).T\n","plot_with_decision_boundary(net, X, y)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.10"}},"nbformat":4,"nbformat_minor":0}
