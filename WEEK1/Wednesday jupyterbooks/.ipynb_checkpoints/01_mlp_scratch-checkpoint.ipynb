{"cells":[{"cell_type":"markdown","metadata":{"id":"Gvlti7HM9DGQ"},"source":["The following additional libraries are needed to run this\n","notebook. Note that running on Colab is experimental, please report a Github\n","issue if you have any problem."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pbedl7mJ9DGU"},"outputs":[],"source":["!pip install d2l==0.16"]},{"cell_type":"markdown","metadata":{"id":"1DsAzhtt9DGU","origin_pos":0},"source":["# Implementation of Multilayer Perceptrons from Scratch\n","\n","In this notebook, you will learn how to implement a \n","multilayer perceptron for classification. \n","To compare against our previous results\n","achieved with the perceptron in the previous tutorial,\n","we will continue to work with\n","the Fashion-MNIST image classification dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LO3ytTbd9DGV","origin_pos":2,"tab":["pytorch"]},"outputs":[],"source":["from d2l import torch as d2l\n","import torch\n","from torch import nn\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-vTZ79Dc9DGV","origin_pos":4,"tab":["pytorch"]},"outputs":[],"source":["batch_size = 256\n","train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"]},{"cell_type":"markdown","source":["Let's look at a few images from the first batch:"],"metadata":{"id":"SKj73VNBR3CV"}},{"cell_type":"code","source":["X, y = next(iter(train_iter))\n","\n","\n","rows, cols = 2,5\n","\n","plt.figure(figsize=(10,5))\n","for i in range(rows*cols):\n","  plt.subplot(rows, cols, i+1)\n","  img_id = np.random.randint(batch_size)\n","  plt.imshow(X[img_id,0,:,:])\n","  plt.axis('off')\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"kDTE_OrgPpTq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AuK8RybE9DGW","origin_pos":5},"source":["## Initializing Model Parameters\n","\n","Recall that Fashion-MNIST contains 10 classes,\n","and that each image consists of a $28 \\times 28 = 784$\n","grid of grayscale pixel values.\n","Again, we will disregard the spatial structure\n","among the pixels for now,\n","so we can think of this as simply a classification dataset\n","with 784 input features and 10 classes.\n","\n","To begin, we will implement an MLP\n","with one hidden layer and 256 hidden units.\n","Note that we can regard both of these quantities\n","as hyperparameters.\n","Typically, we choose layer widths in powers of 2,\n","which tend to be computationally efficient because\n","of how memory is allocated and addressed in hardware.\n","\n","The figure below illustrates the network architecture. The trainable parameters of the network are the weights W1 connecting the input layer with the hidden layer and the weights W2 connecting the hidden layer with the output layer. Note that appart from the weights, the neurons in the hidden and output layers also have a bias, which is not illustrated in the figure. \n","<center><img src=\"https://git.wur.nl/koots006/wias-course-image-and-video-analysis/-/raw/main/day3/classification_network_784_256_10.png\" width=400></center>\n","\n","\n","We will represent our parameters with several tensors.\n","Note that *for every layer*, we must keep track of\n","one weight matrix and one bias vector.\n","As always, we allocate memory\n","for the gradients of the loss with respect to these parameters.\n","\n"]},{"cell_type":"markdown","source":["**Exercise**:\n","* Calculate by hand how many weights there are in `W1` and `W2`. And how many biases are there for the hidden layer (b1) and the output layer (b2)?\n","* Run the code below\n","* Get the size of the tensors `W1`, `b1`, `W2` and `b2` using `W1.shape`, etc\n","* How many trainable parameters are there in total?\n","* Check the content of the tensor `W1` using `print(W1)`. What do you see? How are these values determined? Look into the code to see how W1 was determined."],"metadata":{"id":"B3uK4UJTnJU6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"kE4ou4UG9DGW","origin_pos":7,"tab":["pytorch"]},"outputs":[],"source":["num_inputs, num_outputs, num_hiddens = 784, 10, 256\n","\n","W1 = nn.Parameter(torch.randn(\n","    num_inputs, num_hiddens, requires_grad=True) * 0.01)\n","b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))\n","W2 = nn.Parameter(torch.randn(\n","    num_hiddens, num_outputs, requires_grad=True) * 0.01)\n","b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))\n","\n","params = [W1, b1, W2, b2]"]},{"cell_type":"code","source":["## TODO: add code to get the size of the tensors\n",".."],"metadata":{"id":"98se54YKnMBx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TShsAZrJ9DGX","origin_pos":9},"source":["## Activation Function\n","\n","To make sure we know how everything works,\n","we will implement the ReLU activation ourselves\n","using the maximum function rather than\n","invoking the built-in `relu` function directly.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oDwqQr339DGX","origin_pos":11,"tab":["pytorch"]},"outputs":[],"source":["def relu(X):\n","    a = torch.zeros_like(X)\n","    return torch.max(X, a)"]},{"cell_type":"markdown","source":["**Exercise:**\n","* Test the `relu` function by adding some code in the block below. You need to provide a torch Tensor as input. For instance: \n","`relu(torch.Tensor([-2, 0, 2])`\n","* Check if the behavior of the activation function is correct, that is, negative values for X should all get an output of 0.0 and positive values for X should just return that value"],"metadata":{"id":"jyF3x17nnQxk"}},{"cell_type":"code","source":["## TODO: Add code to test the relu function\n",".."],"metadata":{"id":"8nwRhmVdnRLe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B48aIgie9DGY","origin_pos":13},"source":["## Model\n","\n","Using the definition abobe, we will now construt the neural network model for this MLP. A few step that we need to take:\n","\n","1. The images in the dataset are 28 x 28, but for this MLP, we do not consider the 2D spatial structure of the images, but just use it as a 28x28=784 element input vector, `X`. To get this, we need to `reshape` each two-dimensional image into\n","a flat vector of length  `num_inputs`.\n","\n","2. We have to calculate the activation of the neurons in the hidden layer by multiplying the input vector, `X1` with the weights of the first layer `W1`, adding the biases `b1` and applying the relu activation function using<br>\n","```H = relu(X @ W1 + b1)```, where `@` implements the matrix multiplication.\n","\n","3. With the activations of the hidden layer, we can calculate the activations of the neurons in the output layer in a similar way, but now without using relu using:<br>\n","```O = H @ W2 + b2```\n","\n","The code below implements this network:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"46MrECmb9DGY","origin_pos":15,"tab":["pytorch"]},"outputs":[],"source":["def net(X):\n","    X = X.reshape((-1, num_inputs))\n","    H = relu(X @ W1 + b1)  # Here '@' stands for matrix multiplication\n","    O = H @ W2 + b2\n","    return (O)"]},{"cell_type":"markdown","source":["To test the function, we can put a few images from the training set as input to observe the output:"],"metadata":{"id":"F8ER9LK5VSby"}},{"cell_type":"code","source":["X, y = next(iter(train_iter))\n","\n","img_id = 0\n","pred = net(X[img_id])\n","\n","pred"],"metadata":{"id":"9xyaI61zPjNd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Remember that the output of the networks gives an activation for every class. We can find the class with the maximum activation using `argmax()` and compare it to the ground truth. As this network has not been trained yet and the weights are initialized with random values, this prediction is most likely wrong:"],"metadata":{"id":"MxHdk83hV1kO"}},{"cell_type":"code","source":["print('Predicted class:',pred.argmax().numpy())\n","print('True class:',y[0].numpy())"],"metadata":{"id":"aroRDsjlWo3Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise:**\n","* Below, we test and evaluate the untrained MLP. What do you think the accuracy will be?\n","* Run the code to check it."],"metadata":{"id":"edu2zlHClaFX"}},{"cell_type":"code","source":["d2l.predict_ch3(net, test_iter)\n","acc_test = d2l.evaluate_accuracy(net, test_iter)\n","print(\"The accuracy on the test set is: \", acc_test)"],"metadata":{"id":"aG6pPC6RlGpk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IQjOc2_F9DGY","origin_pos":17},"source":["## Loss Function\n","\n","Remember that for classification, we need to use cross-entropy loss based on the softmax of the network ouput. To ensure numerical stability,\n","and because we already implemented\n","the softmax function from scratch in the previous tutorial,\n","we use the function `nn.CrossEntropyLoss()` in pyTorch\n","for calculating the **softmax** and **cross-entropy loss**:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JvtGA1Xi9DGY","origin_pos":19,"tab":["pytorch"]},"outputs":[],"source":["loss = nn.CrossEntropyLoss()"]},{"cell_type":"markdown","metadata":{"id":"zJcOo9T59DGZ","origin_pos":21},"source":["## Training\n","\n","Fortunately, the training loop for MLPs\n","is exactly the same as for the perceptron.\n","Leveraging the `d2l` package again,\n","we call the `train_ch3` function,\n","setting the number of epochs to 10\n","and the learning rate to 0.1."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eK8SyJsM9DGa","origin_pos":23,"tab":["pytorch"]},"outputs":[],"source":["num_epochs, lr = 10, 0.1\n","updater = torch.optim.SGD(params, lr=lr)\n","d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)"]},{"cell_type":"markdown","metadata":{"id":"5gwVV8bj9DGc","origin_pos":25},"source":["**Exercise:**\n","* Run the code below to evaluate the learned model on the test data.\n","* Do you see an improvement over the results you gained yesterday?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"869bAFmZ9DGc","origin_pos":26,"tab":["pytorch"]},"outputs":[],"source":["d2l.predict_ch3(net, test_iter)\n","acc_test = d2l.evaluate_accuracy(net, test_iter)\n","print(\"The accuracy on the test set is: \", acc_test)"]},{"cell_type":"markdown","metadata":{"id":"Yj1-4PkG9DGc","origin_pos":27},"source":["## Summary\n","\n","* We saw that implementing a simple MLP is easy, even when done manually.\n","* However, with a large number of layers, implementing MLPs from scratch can still get messy (e.g., naming and keeping track of our model's parameters).\n","\n","\n","## Exercises\n","\n","1. Create two additional networks and compare the results against the current network:\n"," -  (A) A network without hidden layers (aka the perceptron) (Note that the output layer should not use ReLU)\n"," -  (B) A network with two hidden layers with in both layers 64 neurons (Note that both hidden layers should use a non-linear activation function such as ReLU)\n","\n"," NB. You need to change a few things:\n","  - Set the right model parameters as in the section ***Initializing Model Parameters***\n","  - Define the network as in the section ***The model***\n","  - Train the new networks and write down the test loss as in the section  ***Training***\n","  - Possibly, you need to change the `learning rate` \n"," \n","2. The network that we now used, applied the ReLU activation function to the neurons in the hidden layer.\n","  - Use `torch.sigmoid()` to build an alternative network that applies the sigmoid function to the neurons in the hidden layer.\n","  -  Is there a difference in performance or training time?\n","\n","**NOTE:** While training the network in these exercises you might face an assertion error at the end of the training process pointing that the `train_loss` is larger than 0.5. No worries, there's nothing wrong with your code. It's just that the d2l team decided to put an assertion for the function `train_ch3` that warns you if the loss at the end of the training process is larger than 0.5 (see [source code](https://github.com/d2l-ai/d2l-en/blob/4fce3dfce2cacdb120a8cc5f0dc7e31841833233/d2l/torch.py#L334). To elimitate the error just train again with a slightly larger learning rate.\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"BVoBzGdntVHG"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.10"}},"nbformat":4,"nbformat_minor":0}