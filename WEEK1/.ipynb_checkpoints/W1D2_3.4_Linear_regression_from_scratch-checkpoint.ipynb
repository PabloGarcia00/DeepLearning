{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11881,
     "status": "ok",
     "timestamp": 1709800191664,
     "user": {
      "displayName": "rens holmer",
      "userId": "11112399563567657550"
     },
     "user_tz": -60
    },
    "id": "TFWgkYYtHGWh",
    "outputId": "a006875f-94ca-46ea-b27c-85b32857a75d"
   },
   "outputs": [],
   "source": [
    "!pip install d2l==0.16.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNcWUjA5HGWi",
    "origin_pos": 0
   },
   "source": [
    "# Linear regression from scratch\n",
    "\n",
    "Now that you understand the key ideas behind gradient descent optimization, we can begin to work through a hands-on implementation of linear regression in code. In this section, we will implement the entire method from scratch, including the data pipeline, the model, the loss function, and the minibatch stochastic gradient descent optimizer. While modern deep learning frameworks can automate nearly all of this work, implementing things from scratch is the only way to make sure that you really know what you are doing. Moreover, when it comes time to customize models, defining our own layers or loss functions, understanding how things work under the hood will prove handy. In this section, we will rely only on tensors and auto differentiation; in the next notebook, you will see a more concise implementation, taking advantage of the Pytorch deep learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9294,
     "status": "ok",
     "timestamp": 1709800200956,
     "user": {
      "displayName": "rens holmer",
      "userId": "11112399563567657550"
     },
     "user_tz": -60
    },
    "id": "l9HwNkeVHGWj",
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhxF_FC2HGWk",
    "origin_pos": 4
   },
   "source": [
    "## Generating the dataset\n",
    "\n",
    "To keep things simple, we will construct an artificial dataset according to a linear model with additive noise.\n",
    "Our task will be to recover this model's parameters using the finite set of examples contained in our dataset.\n",
    "We will keep the data low-dimensional so we can visualize it easily. In the following code snippet, we generate a dataset containing 1,000 examples, each consisting of 2 features sampled from a standard normal distribution.\n",
    "Thus our synthetic dataset will be a matrix $\\mathbf{X}\\in \\mathbb{R}^{1000 \\times 2}$.\n",
    "\n",
    "The true parameters generating our dataset will be $\\mathbf{w} = [2, -3.4]^\\top$ and $b = 4.2$,\n",
    "and our synthetic labels will be assigned according to the following linear model with the noise term $\\epsilon$:\n",
    "\n",
    "$$\\mathbf{y}= \\mathbf{X} \\mathbf{w} + b + \\mathbf\\epsilon.$$\n",
    "\n",
    "You could think of $\\epsilon$ as capturing potential measurement errors on the features and labels. We will assume that the standard assumptions hold and thus that $\\epsilon$ obeys a normal distribution with mean of 0.\n",
    "To make our problem easy, we will set its standard deviation to 0.01. The following code generates our synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJN2keCIHGWk",
    "origin_pos": 5,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def synthetic_data(w, b, num_examples):\n",
    "    \"\"\"Generate y = Xw + b + noise.\"\"\"\n",
    "    X = torch.normal(0, 1, (num_examples, len(w)))\n",
    "    y = torch.matmul(X, w) + b\n",
    "    y += torch.normal(0, 0.01, y.shape)\n",
    "    return X, y.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ddRdTakDHGWl",
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9BWA119HGWl",
    "origin_pos": 8
   },
   "source": [
    "Note that each row in `features` consists of a 2-dimensional data example\n",
    "and that each row in `labels` consists of a 1-dimensional label value (a scalar).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "error",
     "timestamp": 1709800172259,
     "user": {
      "displayName": "rens holmer",
      "userId": "11112399563567657550"
     },
     "user_tz": -60
    },
    "id": "rQs6luFAHGWl",
    "origin_pos": 9,
    "outputId": "f3fa77db-2b23-4472-b8ee-f74f160fd20f",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "print('features:', features[0],'\\nlabel:', labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EojQUA6HGWm",
    "origin_pos": 10
   },
   "source": [
    "By generating a scatter plot using the second feature `features[:, 1]` and `labels`,\n",
    "we can clearly observe the linear correlation between the two.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4EkdCE8sHGWm",
    "origin_pos": 11,
    "outputId": "af63f2b6-6171-4aa1-ea67-921668a92f04",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "d2l.set_figsize()\n",
    "d2l.plt.scatter(features[:, (1)].detach().numpy(),\n",
    "                labels.detach().numpy(), 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOyOF8lVHGWn",
    "origin_pos": 12
   },
   "source": [
    "## Reading the dataset\n",
    "\n",
    "Recall that training models consists of making multiple passes over the dataset, grabbing one minibatch of examples at a time, and using them to update our model. Since this process is so fundamental to training machine learning algorithms, it is worth defining a utility function to shuffle the dataset and access it in minibatches.\n",
    "\n",
    "In the following code, we define the `data_iter` function to demonstrate one possible implementation of this functionality. The function takes a batch size, a matrix of features, and a vector of labels, yielding minibatches of the size `batch_size`. Each minibatch consists of a tuple of features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yEyRvZt3HGWn",
    "origin_pos": 13,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # The examples are read at random, in no particular order\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = torch.tensor(\n",
    "            indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]\n",
    "        # If you don't know about 'yield', check\n",
    "        # https://www.geeksforgeeks.org/use-yield-keyword-instead-return-keyword-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqZg5M3YHGWo",
    "origin_pos": 15
   },
   "source": [
    "In general, note that we want to use reasonably sized minibatches to take advantage of the GPU hardware, which excels at parallelizing operations. Because each example can be fed through our models in parallel and the gradient of the loss function for each example can also be taken in parallel, GPUs allow us to process hundreds of examples in scarcely more time than it might take to process just a single example.\n",
    "\n",
    "To build some intuition, let us read and print the first small batch of data examples. The shape of the features in each minibatch tells us both the minibatch size and the number of input features. Likewise, our minibatch of labels will have a shape given by `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L76MmhxdHGWo",
    "origin_pos": 16,
    "outputId": "c8c68a29-5ced-4af6-f33d-2bbf6550ab74",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\n",
    "for X, y in data_iter(batch_size, features, labels):\n",
    "    print(X, '\\n', y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GoO2c46JPCe"
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "If the number of examples cannot be divided by the batch size, what happens to the `data_iter` function's behavior?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f81WAFeFHGWp",
    "origin_pos": 17
   },
   "source": [
    "## Initializing model parameters\n",
    "\n",
    "Before we can begin optimizing our model's parameters by minibatch stochastic gradient descent, we need to have some parameters in the first place. In the following code, we initialize weights by sampling random numbers from a normal distribution with mean 0 and a standard deviation of 0.01, and setting the bias to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kSHtLnQ5HGWq",
    "origin_pos": 19,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yR4T2m8Jtjb"
   },
   "source": [
    "### Exercise 2\n",
    "\n",
    "Reason what would happen if we were to initialize the weights $\\mathbf{w}$ using a very high value. After going through the code below, you can get back to the initialization above and try yourself what happens with high values for the weights at the start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AE4G6HTZHGWr",
    "origin_pos": 21
   },
   "source": [
    "After initializing our parameters, our next task is to update them until they fit our data sufficiently well.\n",
    "Each update requires taking the gradient of our loss function with respect to the parameters. This is calculated using automatic differentiation. Given this gradient, we can update each parameter in the direction that may reduce the loss.\n",
    "\n",
    "## Defining the model\n",
    "\n",
    "Next, we must define our model, relating its inputs and parameters to its outputs. Recall that to calculate the output of the linear model, we simply take the matrix-vector dot product of the input features $\\mathbf{X}$ and the model weights $\\mathbf{w}$, and add the offset $b$ to each example. Note that below $\\mathbf{Xw}$  is a vector and $b$ is a scalar. Note that when we add a vector and a scalar, broadcasting ensures that the scalar is added to each component of the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4sVrXeNHGWr",
    "origin_pos": 22,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def linreg(X, w, b):\n",
    "    \"\"\"The linear regression model.\"\"\"\n",
    "    return torch.matmul(X, w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFaaYTomHGWr",
    "origin_pos": 23
   },
   "source": [
    "## Defining the loss function\n",
    "\n",
    "Since updating our model requires taking the gradient of our loss function, we ought to define the loss function first. Here we will use the squared loss function. In the implementation, we need to transform the true value `y` into the predicted value's shape `y_hat`. The result returned by the following function\n",
    "will also have the same shape as `y_hat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LrJLNuGeHGWs",
    "origin_pos": 24,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def squared_loss(y_hat, y):\n",
    "    \"\"\"Squared loss.\"\"\"\n",
    "    return (y_hat - y.reshape(y_hat.shape))**2 / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JYu2aF9HGWs",
    "origin_pos": 25
   },
   "source": [
    "## Defining the optimization algorithm\n",
    "\n",
    "At each step, using one minibatch randomly drawn from our dataset, we will estimate the gradient of the loss with respect to our parameters. Next, we will update our parameters in the direction that may reduce the loss. The following code applies the minibatch stochastic gradient descent update, given a set of parameters, a learning rate, and a batch size. The size of the update step is determined by the learning rate `lr`. Because our loss is calculated as a sum over the minibatch of examples, we normalize our step size by the batch size (`batch_size`), so that the magnitude of a typical step size does not depend heavily on our choice of the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8gDQYMLHHGWs",
    "origin_pos": 27,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):\n",
    "    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxKL68FaHGWt",
    "origin_pos": 29
   },
   "source": [
    "## Training\n",
    "\n",
    "Now that we have all of the parts in place, we are ready to implement the main training loop. It is crucial that you understand this code because you will see nearly identical training loops over and over again throughout your career in deep learning.\n",
    "\n",
    "In each iteration, we will grab a minibatch of training examples, and pass them through our model to obtain a set of predictions. After calculating the loss, we initiate the backwards pass through the network, storing the gradients with respect to each parameter. Finally, we will call the optimization algorithm `sgd` to update the model parameters.\n",
    "\n",
    "In summary, we will execute the following loop:\n",
    "\n",
    "* Initialize parameters $(\\mathbf{w}, b)$\n",
    "* Repeat until done\n",
    "    * Compute gradient $\\mathbf{g} \\leftarrow \\partial_{(\\mathbf{w},b)} \\frac{1}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} l(\\mathbf{x}^{(i)}, y^{(i)}, \\mathbf{w}, b)$\n",
    "    * Update parameters $(\\mathbf{w}, b) \\leftarrow (\\mathbf{w}, b) - \\eta \\mathbf{g}$\n",
    "\n",
    "In each *epoch*, we will iterate through the entire dataset (using the `data_iter` function) once passing through every example in the training dataset (assuming that the number of examples is divisible by the batch size). The number of epochs `num_epochs` and the learning rate `lr` are both hyperparameters, which we set here to 5 and 0.03, respectively. Unfortunately, setting hyperparameters is tricky and requires some adjustment by trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6gzlKsbHGWt",
    "origin_pos": 30,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "lr = 0.03\n",
    "num_epochs = 5\n",
    "net = linreg\n",
    "loss = squared_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-DJjdB_zHGWu",
    "origin_pos": 32,
    "outputId": "4369ed59-96d7-41c3-8c44-2f10f2aaed35",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        l = loss(net(X, w, b), y).sum()  # Minibatch loss in `X` and `y`\n",
    "        # Compute gradient on `l` with respect to [`w`, `b`]\n",
    "        l.backward()\n",
    "        sgd([w, b], lr, batch_size)  # Update parameters using their gradient\n",
    "    with torch.no_grad():\n",
    "        train_l = loss(net(features, w, b), labels)\n",
    "        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_nfg_RBHGWu",
    "origin_pos": 34
   },
   "source": [
    "In this case, because we synthesized the dataset ourselves, we know precisely what the true parameters are. Thus, we can evaluate our success in training by comparing the true parameters with those that we learned through our training loop. Indeed they turn out to be very close to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ytCMIzYUHGWu",
    "origin_pos": 35,
    "outputId": "8035599c-ecfe-4e51-b070-6d2af5210b63",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "print(f'error in estimating w: {true_w - w.reshape(true_w.shape)}')\n",
    "print(f'error in estimating b: {true_b - b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fN1lvmGhKOOA"
   },
   "source": [
    "### Exercise 3\n",
    "\n",
    "Experiment using different learning rates to find out how fast the loss function value drops. Note: if you just rerun the code immediately above, you will seem to have a very low loss already at the start... but that is because you then use the previously found estimate for `w` and `b` as starting point! Think about what you should do to prevent this."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
