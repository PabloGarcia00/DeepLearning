{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_arhPUkn4yYA"
   },
   "source": [
    "#**Week 2: Convolutional** **nets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWKNIe5b5CqI"
   },
   "source": [
    "# Part I: Rescaling operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KYbPjuLTnhJA",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhtgr9uhHeXv"
   },
   "source": [
    "## Exercise 1. Add padding manually!\n",
    "\n",
    "(exercise in the slides, [7.3.1](http://d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html#padding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "wXG7-qZqk2GD",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# This is the original image\n",
    "image = torch.tensor([[3,1,4,3],\n",
    "                      [3,2,3,1],\n",
    "                      [4,3,6,4],\n",
    "                      [3,3,1,7]])\n",
    "\n",
    "# Add some padding manually\n",
    "padded_image = nn.functional.pad(image, (1,1,1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0],\n",
       "        [0, 3, 1, 4, 3, 0],\n",
       "        [0, 3, 2, 3, 1, 0],\n",
       "        [0, 4, 3, 6, 4, 0],\n",
       "        [0, 3, 3, 1, 7, 0],\n",
       "        [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_35LWxvK8ju"
   },
   "source": [
    "## Exercise 2: Explore padding and stride with `Conv2D`\n",
    "### Ex. 2a: Explore padding\n",
    "\n",
    "Declare a filter with kernel size 3 and zero padding using nn.conv2D().\n",
    "- Apply it to the image *I* below\n",
    "- Check the shape of the output\n",
    "\n",
    "- Repeat the same, but now using padding 1. What are the dimentions of the output now?\n",
    "\n",
    "- What will happen if padding is 2?\n",
    "\n",
    "- Play with different sizes of kernels and padding.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "3RzjB4uf6mUm",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), bias=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I = torch.tensor([[3,1,4,3],\n",
    "                  [3,2,3,1],\n",
    "                  [4,3,6,4],\n",
    "                  [3,3,1,7]], dtype=torch.float32).reshape((1,1,4,4))\n",
    "\n",
    "nopadding = nn.Conv2d(1,1, kernel_size=3, bias=False, padding=0)\n",
    "# inspect no padding\n",
    "nopadding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yi1qZDMeJlOe"
   },
   "source": [
    "Check the output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "Kx5wgeHWHUKN",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2, 2])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nopadding(I).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "Q0AXzIXxJzuH",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 4, 4])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now repeat with padding 1\n",
    "padding_one = nn.Conv2d(1,1, kernel_size=3, bias=False, padding=1)\n",
    "padding_one(I).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "62iSNwAlnhJC",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 6, 6])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now repeat with padding 1\n",
    "padding_two = nn.Conv2d(1,1, kernel_size=3, bias=False, padding=2)\n",
    "padding_two(I).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I53HJcBdKTgH"
   },
   "source": [
    "### Ex 2b: Explore stride with Conv2D\n",
    "\n",
    "Using the filter of the previous exercise give stride various values, and inspect the resulting shapes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "5SP3Gj8O6oEr",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4, 4])\n",
      "torch.Size([1, 1, 2, 2])\n",
      "torch.Size([1, 1, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "# declare a filter with 1 padding and 1 stride\n",
    "# apply it and print the shape of the output\n",
    "\n",
    "filter_p1_s1 = nn.Conv2d(1,1, kernel_size=3, bias=False, padding=1, stride=1)\n",
    "print(filter_p1_s1(I).shape)\n",
    "\n",
    "# declare the same filter with 1 padding and 2 stride\n",
    "# apply it and print the shape of the output\n",
    "\n",
    "filter_p1_s2 = nn.Conv2d(1,1, kernel_size=3, bias=False, padding=1, stride=2)\n",
    "print(filter_p1_s2(I).shape)\n",
    "\n",
    "# declare the same filter with 1 padding and (2,1) stride (uneven)\n",
    "# apply it and print the shape of the output\n",
    "\n",
    "filter_p1_s21 = nn.Conv2d(1,1, kernel_size=3, bias=False, padding=(2,1), stride=4)\n",
    "print(filter_p1_s21(I).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dA5Ffyrp5Y7X"
   },
   "source": [
    "### Exercise 3: Pooling\n",
    "Inspired from book example [7.5.1](http://d2l.ai/chapter_convolutional-neural-networks/pooling.html#padding-and-stride)\n",
    "\n",
    "By default, the stride and the pooling window *have the same shape* in the PyTorch built-in class `nn.MaxPool2d`.  \n",
    "\n",
    "For example, with `nn.MaxPool2d(2)`, we define a pooling window of shape (2, 2), so we get a stride shape of (2, 2) by default.\n",
    "\n",
    "Of course, we can specify an arbitrary rectangular pooling window and specify the padding and stride for height and width, respectively. But this is not used commonly. As in `nn.MaxPool2d(3, padding=1, stride=2)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "pwubnq_C_6ZS",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 5.,  7.],\n",
      "          [13., 15.]]]])\n",
      "torch.Size([1, 1, 2, 2])\n",
      "tensor([[[[10.]]]])\n",
      "torch.Size([1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Lets create a sample tensor\n",
    "X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))\n",
    "\n",
    "# First we apply a Max Pooling of size 2\n",
    "fpool2d = nn.MaxPool2d(2)\n",
    "print(fpool2d(X))\n",
    "print(fpool2d(X).shape)\n",
    "\n",
    "# Repeat similarily with average pooling\n",
    "# the function is AvgPool2d\n",
    "fpool2d = nn.MaxPool2d(3)\n",
    "print(fpool2d(X))\n",
    "print(fpool2d(X).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vibnK-XnvMI3"
   },
   "source": [
    "Great! You made it!\n",
    "\n",
    "Time to build your first convolutional network :)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
