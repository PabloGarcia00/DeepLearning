{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZE_g1yBi5xv"
      },
      "source": [
        "The following additional libraries are needed to run this\n",
        "notebook. Note that running on Colab is experimental, please report a Github\n",
        "issue if you have any problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qc-YQkiMi5x2"
      },
      "outputs": [],
      "source": [
        "!pip install d2l==0.16.1 --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47v8duZoi5yG"
      },
      "source": [
        "# Fine Tuning\n",
        "\n",
        "In earlier chapters, we discussed how to train models on the Fashion-MNIST training dataset, which only has 60,000 images. We also described ImageNet, the most widely used large-scale image dataset in the academic world, with more than 10 million images and objects of over 1000 categories. However, the size of datasets that we often deal with is usually larger than the first, but smaller than the second.\n",
        "\n",
        "Assume we want to identify different kinds of chairs in images and then push the purchase link to the user. One possible method is to first find a hundred common chairs, take one thousand different images with different angles for each chair, and then train a classification model on the collected image dataset. Although this dataset may be larger than Fashion-MNIST, the number of examples is still less than one tenth of ImageNet. This may result in the overfitting of the complicated model applicable to ImageNet on this dataset. At the same time, because of the limited amount of data, the accuracy of the final trained model may not meet the practical requirements.\n",
        "\n",
        "In order to deal with the above problems, an obvious solution is to collect more data. However, collecting and labeling data can consume a lot of time and money. For example, in order to collect the ImageNet datasets, researchers have spent millions of dollars of research funding. Although, recently, data collection costs have dropped significantly, the costs still cannot be ignored.\n",
        "\n",
        "Another solution is to apply transfer learning to migrate the knowledge learned from the source dataset to the target dataset. For example, although the images in ImageNet are mostly unrelated to chairs, models trained on this dataset can extract more general image features that can help identify edges, textures, shapes, and object composition. These similar features may be equally effective for recognizing a chair.\n",
        "\n",
        "In this section, we introduce a common technique in transfer learning: fine tuning. Fine tuning consists of the following four steps:\n",
        "\n",
        "1. Pre-train a neural network model, i.e., the source model, on a source dataset (e.g., the ImageNet dataset).\n",
        "2. Create a new neural network model, i.e., the target model. This replicates all model designs and their parameters on the source model, except the output layer. We assume that these model parameters contain the knowledge learned from the source dataset and this knowledge will be equally applicable to the target dataset. We also assume that the output layer of the source model is closely related to the labels of the source dataset and is therefore not used in the target model.\n",
        "3. Add an output layer whose output size is the number of target dataset categories to the target model, and randomly initialize the model parameters of this layer.\n",
        "4. Train the target model on a target dataset, such as a chair dataset. We will train the output layer from scratch, while the parameters of all remaining layers are fine tuned based on the parameters of the source model.\n",
        "\n",
        "![Fine tuning. ](http://d2l.ai/_images/finetune.svg)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Hot Dog Recognition\n",
        "\n",
        "Next, we will use a specific example for practice: hot dog recognition. We will fine tune the ResNet model trained on the ImageNet dataset based on a small dataset. This small dataset contains thousands of images, some of which contain hot dogs. We will use the model obtained by fine tuning to identify whether an image contains a hot dog.\n",
        "\n",
        "First, import the packages and modules required for the experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "1"
        },
        "id": "qfnNukapi5yJ"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from d2l import torch as d2l\n",
        "from torch import nn\n",
        "import torch\n",
        "import torchvision\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ikPuSai5yS"
      },
      "source": [
        "### Obtaining the Dataset\n",
        "\n",
        "The hot dog dataset we use was taken from online images and contains $1,400$ positive images containing hot dogs and same number of negative images containing other foods. $1,000$ images of various classes are used for training and the rest are used for testing.\n",
        "\n",
        "We first download the compressed dataset and get two folders `hotdog/train` and `hotdog/test`. Both folders have `hotdog` and `not-hotdog` category subfolders, each of which has corresponding image files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "2"
        },
        "id": "2QRHSb5Ii5yW"
      },
      "outputs": [],
      "source": [
        "# Saved in the d2l package for later use\n",
        "d2l.DATA_HUB['hotdog'] = (d2l.DATA_URL+'hotdog.zip', \n",
        "                         'fba480ffa8aa7e0febbb511d181409f899b9baa5')\n",
        "\n",
        "data_dir = d2l.download_extract('hotdog')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eqp306y-i5yg"
      },
      "source": [
        "We create two `ImageFolderDataset` instances to read all the image files in the training dataset and testing dataset, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "3"
        },
        "id": "dEBa8vJMi5yj"
      },
      "outputs": [],
      "source": [
        "train_imgs = torchvision.datasets.ImageFolder(\n",
        "    os.path.join(data_dir, 'train'))\n",
        "test_imgs = torchvision.datasets.ImageFolder(\n",
        "    os.path.join(data_dir, 'test'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0Iq9NUCi5ys"
      },
      "source": [
        "The first 8 positive examples and the last 8 negative images are shown below. As you can see, the images vary in size and aspect ratio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "4"
        },
        "id": "1lZLwf5_i5yu"
      },
      "outputs": [],
      "source": [
        "hotdogs = [train_imgs[i][0] for i in range(8)]\n",
        "not_hotdogs = [train_imgs[-i - 1][0] for i in range(8)]\n",
        "d2l.show_images(hotdogs + not_hotdogs, 2, 8, scale=1.4);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_8c_O1Di5y2"
      },
      "source": [
        "During training, we first crop a random area with random size and random aspect ratio from the image and then scale the area to an input with a height and width of 224 pixels. During testing, we scale the height and width of images to 256 pixels, and then crop the center area with height and width of 224 pixels to use as the input. In addition, we normalize the values of the three RGB (red, green, and blue) color channels. The average of all values of the channel is subtracted from each value and then the result is divided by the standard deviation of all values of the channel to produce the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "5"
        },
        "id": "w7HueouWi5y4"
      },
      "outputs": [],
      "source": [
        "# We specify the mean and variance of the three RGB channels to normalize the\n",
        "# image channel\n",
        "normalize = torchvision.transforms.Normalize(\n",
        "    [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "\n",
        "train_augs = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.RandomResizedCrop(224),\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    normalize])\n",
        "\n",
        "test_augs = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize(256),\n",
        "    torchvision.transforms.CenterCrop(224),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    normalize])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ank7sNWWi5zA"
      },
      "source": [
        "### Defining and Initializing the Model\n",
        "\n",
        "We use ResNet-18, which was pre-trained on the ImageNet dataset, as the source model. Here, we specify `pretrained=True` to automatically download and load the pre-trained model parameters. The first time they are used, the model parameters need to be downloaded from the Internet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "6"
        },
        "id": "7cdKdzYei5zC"
      },
      "outputs": [],
      "source": [
        "pretrained_net = torchvision.models.resnet18(pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55cWkZSui5zI"
      },
      "source": [
        "The pre-trained source model instance contains two member variables: `features` and `output`. The former contains all layers of the model, except the output layer, and the latter is the output layer of the model. The main purpose of this division is to facilitate the fine tuning of the model parameters of all layers except the output layer. The member variable `output` of source model is given below. As a fully connected layer, it transforms ResNet's final global average pooling layer output into 1000 class output on the ImageNet dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "7"
        },
        "id": "iPNDVOE9i5zK"
      },
      "outputs": [],
      "source": [
        "pretrained_net.fc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-gTxuifi5zT"
      },
      "source": [
        "We then build a new neural network to use as the target model. It is defined in the same way as the pre-trained source model, but the final number of outputs is equal to the number of categories in the target dataset. In the code below, the model parameters in the member variable `features` of the target model instance `finetune_net` are initialized to model parameters of the corresponding layer of the source model. Because the model parameters in `features` are obtained by pre-training on the ImageNet dataset, it is good enough. Therefore, we generally only need to use small learning rates to \"fine tune\" these parameters. In contrast, weights in the member variable `output` are randomly initialized and generally require a larger learning rate to learn from scratch. Assume the learning rate in the `Trainer` instance is $\\eta$ and use a learning rate of $10\\eta$ to update the model parameters in the member variable `output`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "8"
        },
        "id": "Hg84aExti5zW"
      },
      "outputs": [],
      "source": [
        "finetune_net = torchvision.models.resnet18(pretrained=True)\n",
        "finetune_net.fc = nn.Linear(finetune_net.fc.in_features, 2)\n",
        "nn.init.xavier_uniform_(finetune_net.fc.weight);\n",
        "# If `param_group=True`, the model parameters in fc layer will be updated\n",
        "# using a learning rate ten times greater, defined in the trainer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IexFTKaji5zc"
      },
      "source": [
        "### Fine Tuning the Model\n",
        "\n",
        "We first define a training function `train_fine_tuning` that uses fine tuning so it can be called multiple times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "9"
        },
        "id": "82L3DALgi5ze"
      },
      "outputs": [],
      "source": [
        "def train_fine_tuning(net, learning_rate, batch_size=128, num_epochs=5,\n",
        "                      param_group=True):\n",
        "    train_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(\n",
        "        os.path.join(data_dir, 'train'), transform=train_augs),\n",
        "        batch_size=batch_size, shuffle=True)\n",
        "    test_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(\n",
        "        os.path.join(data_dir, 'test'), transform=test_augs),\n",
        "        batch_size=batch_size)\n",
        "    devices = d2l.try_all_gpus()\n",
        "    loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
        "    if param_group:\n",
        "        params_1x = [param for name, param in net.named_parameters()\n",
        "             if name not in [\"fc.weight\", \"fc.bias\"]]\n",
        "        trainer = torch.optim.SGD([{'params': params_1x},\n",
        "                                   {'params': net.fc.parameters(),\n",
        "                                    'lr': learning_rate * 10}],\n",
        "                                lr=learning_rate, weight_decay=0.001)\n",
        "    else:\n",
        "        trainer = torch.optim.SGD(net.parameters(), lr=learning_rate,\n",
        "                                  weight_decay=0.001)\n",
        "    d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,\n",
        "                   devices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFG4pANWi5zl"
      },
      "source": [
        "We set the learning rate in the `Trainer` instance to a smaller value, such as 5e-5, in order to fine tune the model parameters obtained in pre-training. Based on the previous settings, we will train the output layer parameters of the target model from scratch using a learning rate ten times greater."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "10"
        },
        "id": "ycW36eJXi5zn"
      },
      "outputs": [],
      "source": [
        "train_fine_tuning(finetune_net, 5e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2ivolF8i5z4"
      },
      "source": [
        "For comparison, we define an identical model, but initialize all of its model parameters to random values. Since the entire model needs to be trained from scratch, we can use a larger learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "12"
        },
        "id": "wZg9yqSqi5z6"
      },
      "outputs": [],
      "source": [
        "scratch_net = torchvision.models.resnet18()\n",
        "scratch_net.fc = nn.Linear(scratch_net.fc.in_features, 2)\n",
        "train_fine_tuning(scratch_net, 5e-4, param_group=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDXnGuXNi5z_"
      },
      "source": [
        "As you can see, the fine-tuned model tends to achieve higher precision in the same epoch because the initial values of the parameters are better.\n",
        "\n",
        "\n",
        "## Summary\n",
        "\n",
        "\n",
        "* Transfer learning migrates the knowledge learned from the source dataset to the target dataset. Fine tuning is a common technique for transfer learning.\n",
        "* The target model replicates all model designs and their parameters on the source model, except the output layer, and fine tunes these parameters based on the target dataset. In contrast, the output layer of the target model needs to be trained from scratch.\n",
        "* Generally, fine tuning parameters use a smaller learning rate, while training the output layer from scratch can use a larger learning rate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxwNCqLF5bJ2"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_6RC6VIf5yu"
      },
      "source": [
        "**Exercise 1:** Keep increasing the learning rate of `finetune_net`. How does the precision of the model change?\n",
        "\n",
        "**Exercise 2:** Set the parameters in `finetune_net.parameters` to the parameters of the source model and do not update them during training. What will happen? You can use the following code:\n",
        "```python\n",
        "for param in net.parameters():\n",
        "    param.requires_grad = False\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooyot7CkgLDs"
      },
      "outputs": [],
      "source": [
        "finetune_net_2 = torchvision.models.resnet18(pretrained=True)\n",
        "finetune_net_2.fc = nn.Linear(finetune_net_2.fc.in_features, 2)\n",
        "nn.init.xavier_uniform_(finetune_net_2.fc.weight);\n",
        "\n",
        "### TODO ADD YOUR CODE HERE (2 lines)\n",
        "\n",
        "    \n",
        "train_fine_tuning(finetune_net_2, 5e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c9LkC2Uh9mL"
      },
      "source": [
        "# Saving your training process\n",
        "Google Colab is a great platform which gives you access to expensive GPUs for free. However, this access is limited to 12 continuous hours. Also, you need to have your laptop continuously on and connected to the internet to keep it running. This means that if your laptop is supended, or you lose your internet connection, the training process will be lost.\n",
        "\n",
        "To solve this issue, we recommend you to save the weights of your network while you are taining. So, if Google Colab crashes you don't lose progress. We are going to save the network weights in your own Google Drive.\n",
        "\n",
        "The process is:\n",
        "\n",
        "**1.** *Connect to your google drive*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZiYghjWNz9R"
      },
      "outputs": [],
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWOyWH-Bi6NI"
      },
      "source": [
        "**2.** *Create the directory where you want to save your weights. `mkdir` is a linux command to create a directory.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJ6japwoN2tn"
      },
      "outputs": [],
      "source": [
        "# Create a folder called 'fine_tuning_exercise' in your google drive\n",
        "!mkdir /content/drive/My\\ Drive/fine_tuning_exercise # mkdir is the unix command to create a directory\n",
        "!ls /content/drive/My\\ Drive # ls is the unix command to see what's inside of a directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QPUzobojPpE"
      },
      "source": [
        "**3.** Adapt your training function to save the weights at every epoch.\n",
        "\n",
        "This code allows you to save the weights of a network\n",
        "```python\n",
        "torch.save(net.state_dict(), '/content/drive/My Drive/fine_tuning_exercise/net.params')\n",
        "```\n",
        "\n",
        "**Exercise 3:** Adapt the train_ch13 function to save the weights at every epoch and train a finetuned network with it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17Vlo_x_OnAg"
      },
      "outputs": [],
      "source": [
        "# Saved in the d2l package for later use\n",
        "def train_ch13_saving_weights(net, train_iter, test_iter, loss, trainer, num_epochs,\n",
        "               devices=d2l.try_all_gpus()):\n",
        "    num_batches, timer = len(train_iter), d2l.Timer()\n",
        "    animator = d2l.Animator(xlabel='epoch', xlim=[0, num_epochs], ylim=[0, 1],\n",
        "                            legend=['train loss', 'train acc', 'test acc'])\n",
        "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
        "    for epoch in range(num_epochs):\n",
        "        # Store training_loss, training_accuracy, num_examples, num_features\n",
        "        metric = d2l.Accumulator(4)\n",
        "        for i, (features, labels) in enumerate(train_iter):\n",
        "            timer.start()\n",
        "            l, acc = d2l.train_batch_ch13(\n",
        "                net, features, labels, loss, trainer, devices)\n",
        "            metric.add(l, acc, labels.shape[0], labels.numel())\n",
        "            timer.stop()\n",
        "            if (i+1) % (num_batches // 5) == 0:\n",
        "                animator.add(epoch+i/num_batches,\n",
        "                             (metric[0]/metric[2], metric[1]/metric[3], None))\n",
        "        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
        "        animator.add(epoch+1, (None, None, test_acc))\n",
        "        \n",
        "        ### TODO ADD YOUR CODE HERE\n",
        "        torch.save(\"Write your code here\")\n",
        "\n",
        "    print('loss %.3f, train acc %.3f, test acc %.3f' % (\n",
        "        metric[0]/metric[2], metric[1]/metric[3], test_acc))\n",
        "    print('%.1f examples/sec on %s' % (\n",
        "        metric[2]*num_epochs/timer.sum(), devices))\n",
        "\n",
        "def train_fine_tuning_saving_weights(net, learning_rate, batch_size=128, num_epochs=5,\n",
        "                      param_group=True):\n",
        "    train_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(\n",
        "        os.path.join(data_dir, 'train'), transform=train_augs),\n",
        "        batch_size=batch_size, shuffle=True)\n",
        "    test_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(\n",
        "        os.path.join(data_dir, 'test'), transform=test_augs),\n",
        "        batch_size=batch_size)\n",
        "    devices = d2l.try_all_gpus()\n",
        "    loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
        "    if param_group:\n",
        "        params_1x = [param for name, param in net.named_parameters()\n",
        "             if name not in [\"fc.weight\", \"fc.bias\"]]\n",
        "        trainer = torch.optim.SGD([{'params': params_1x},\n",
        "                                   {'params': net.fc.parameters(),\n",
        "                                    'lr': learning_rate * 10}],\n",
        "                                lr=learning_rate, weight_decay=0.001)\n",
        "    else:\n",
        "        trainer = torch.optim.SGD(net.parameters(), lr=learning_rate,\n",
        "                                  weight_decay=0.001)\n",
        "    train_ch13_saving_weights(net, train_iter, test_iter, loss, trainer, num_epochs, devices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_FgwvO3gyLH"
      },
      "outputs": [],
      "source": [
        "### TODO ADD YOUR CODE HERE (~3 lines)\n",
        "finetune_net_3 = ...\n",
        "\n",
        "\n",
        "# Run your adapted function to check if the weighs are been saved\n",
        "train_fine_tuning_saving_weights(finetune_net_3, 5e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "av0tZGMZHRBv"
      },
      "outputs": [],
      "source": [
        "# Check if the weights have been saved properly\n",
        "!ls /content/drive/My\\ Drive/fine_tuning_exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VReZ7NzkPGz"
      },
      "source": [
        "Additionally this other code allows you to load stored weights to a network. Save it, you might need it in later stages of the course.\n",
        "```python\n",
        "net.load_state_dict(torch.load('/content/drive/My Drive/fine_tuning_exercise/net.params'))\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "week2_02_fine_tuning_pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
