{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_arhPUkn4yYA"
   },
   "source": [
    "# Regularizations with CNNs\n",
    "\n",
    "## Lab 3 Regularization by Initalization, Batch Norm & Dropout\n",
    "\n",
    "Author: M. Ru√üwurm, 2024, based on notebooks from D.Tuia (2020)\n",
    "\n",
    "In this lab, we get the more complex CNN model (from Lab 2) to work by \n",
    "1. changing the weight initialization\n",
    "2. adding batch normalization\n",
    "3. adding dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Setup\n",
    "\n",
    "Let's get the required python packages\n",
    "\n",
    "**d2l** Package:\n",
    "The \"d2l\" (short for \"dive into deep learning\") package is a Python library designed to accompany the book \"Dive into Deep Learning\"\n",
    "\n",
    "**Pytoch**:\n",
    "Pytorch is an open-source machine learning library and scientific computing framework, primarily used for deep learning applications. \n",
    "\n",
    "**sklearn.metrics**:\n",
    "The \"sklearn.metrics\" module is part of the scikit-learn library, a popular machine learning library in Python. The metrics module specifically focuses on providing tools for evaluating the performance of machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T20:52:36.313650Z",
     "start_time": "2024-03-12T20:52:34.447069Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install -q d2l\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "TqGqJ5fk1f3U"
   },
   "source": [
    "## Data - FashionMNIST\n",
    "\n",
    "Let's start by loading FashionMNIST data\n",
    "\n",
    "Fashion MNIST is a dataset used in machine learning and computer vision, serving as a benchmark for image classification tasks. It consists of 70,000 grayscale images of clothing items, categorized into 10 classes such as t-shirts, dresses, and sneakers. Fashion MNIST is a popular alternative to the traditional handwritten digit MNIST dataset, providing a more complex challenge for developing and testing image recognition algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T20:52:38.551056Z",
     "start_time": "2024-03-12T20:52:36.321574Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "q0_l88B-bawo",
    "outputId": "f374a494-f053-4429-81c7-60414870a196"
   },
   "outputs": [],
   "source": [
    "fashionMNIST = d2l.FashionMNIST(batch_size=128)\n",
    "\n",
    "train_dataloader = fashionMNIST.get_dataloader(train=True)\n",
    "val_dataloader = fashionMNIST.get_dataloader(train=False)\n",
    "\n",
    "text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    X,y = batch\n",
    "    fashionMNIST.visualize(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Later, we would like to validate the model. \n",
    "This given function \n",
    "1. iterates through all data in a (validaiton) dataloader\n",
    "2. stores the ground truth (y_true) and predictions (y_pred)\n",
    "3. prints a classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T20:52:38.565850Z",
     "start_time": "2024-03-12T20:52:38.554335Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(model, dataloader):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for X,y in dataloader:\n",
    "        y_true.append(y)\n",
    "        y_pred.append(model(X).argmax(1))\n",
    "        \n",
    "    y_true = torch.hstack(y_true)\n",
    "    y_pred = torch.hstack(y_pred)\n",
    "    \n",
    "    print(classification_report(y_pred=y_pred.numpy(), y_true=y_true.numpy(), labels=torch.arange(10).numpy(), target_names=text_labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOMGikUM4QaY"
   },
   "source": [
    "## Run 1 - simple CNN (LeNet) default initialization\n",
    "\n",
    "Let's create an instance of the LeNet model from Lab 2 again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T12:13:35.433024Z",
     "start_time": "2024-03-13T12:13:35.430748Z"
    },
    "id": "b87FR4_m1f3V"
   },
   "outputs": [],
   "source": [
    "class LeNetCNNModel(d2l.Classifier):\n",
    "    def __init__(self, lr=0.1, weight_decay=1e-4, momentum=0.4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(6, kernel_size=5, padding=2), nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.LazyConv2d(16, kernel_size=5), nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(120), nn.Sigmoid(),\n",
    "            nn.LazyLinear(84), nn.Sigmoid(),\n",
    "            nn.LazyLinear(10))\n",
    "    def training_step(self, batch):\n",
    "        Y_hat = self(*batch[:-1])\n",
    "        loss = self.loss(Y_hat, batch[-1])\n",
    "        self.plot('loss', loss, train=True)\n",
    "        self.plot('acc', self.accuracy(Y_hat, batch[-1]), train=True)\n",
    "        return loss # the package takes care of the\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        Y_hat = self(*batch[:-1])\n",
    "        self.plot('loss', self.loss(Y_hat, batch[-1]), train=False)\n",
    "        self.plot('acc', self.accuracy(Y_hat, batch[-1]), train=False)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=self.lr, \n",
    "                               weight_decay=self.weight_decay)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's train the model again.\n",
    "\n",
    "**Task** \n",
    "* Train the model confirm that it does does not train in its current form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "zQSEPleL1f3V",
    "outputId": "0937cf53-7330-40b5-ed24-9e5f217c56f2"
   },
   "outputs": [],
   "source": [
    "#TODO: Train the model with weight_decay=1e-4, momentum=0.5\n",
    "# model = ...\n",
    "# trainer = ...\n",
    "# trainer.fit(...)\n",
    "\n",
    "\n",
    "\n",
    "model.layer_summary(X_shape=X.shape)\n",
    "validate(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Run 2: Kaiming or Xavier Uniform Initialization of Weights\n",
    "\n",
    "Let's now get the model to train by using a better weight initalization. Check the [Torch documentation](https://pytorch.org/docs/stable/nn.init.html) for details.\n",
    "We give you the code to change the initialization, please modify initializations.\n",
    "\n",
    "**Task**: train the model with different initializations:\n",
    "* replace `kaiming_uniform_` with `xavier_uniform_`\n",
    "* test `kaiming_normal_` with `xavier_normal_`\n",
    "* initialize bias terms with 0 or not, \n",
    "* change the non-linearity function from `sigmoid` to `relu` \n",
    "\n",
    "For more information, check the [Torch documentation](https://pytorch.org/docs/stable/nn.init.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T12:16:48.240569Z",
     "start_time": "2024-03-13T12:16:48.226789Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO try different initializations and observe the training (next cells)\n",
    "def init_cnn(module):\n",
    "    \n",
    "    # Linear Layers\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.kaiming_uniform_(module.weight, nonlinearity=\"sigmoid\")\n",
    "        \n",
    "    # CNN Layers\n",
    "    if type(module) == nn.Conv2d:\n",
    "        nn.init.kaiming_uniform_(module.weight, nonlinearity=\"sigmoid\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LeNetCNNModel(lr=1, weight_decay=1e-4, momentum=0.5)\n",
    "model.layer_summary(X_shape=X.shape)\n",
    "\n",
    "model.apply_init([X], init_cnn)\n",
    "\n",
    "trainer = d2l.Trainer(max_epochs=5, num_gpus=1)\n",
    "trainer.fit(model, fashionMNIST)\n",
    "\n",
    "validate(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Run 3 Model - Batch Normalization\n",
    "\n",
    "Let's improve the model further by adding batch normalization layers between the convolutions and linear transformations\n",
    "\n",
    "**Task**\n",
    "* add `nn.LazyBatchNorm2d()` and `nn.LazyBatchNorm1d()` to the model from above.\n",
    "\n",
    "The layer summary should look like this:\n",
    "```\n",
    "Conv2d output shape:\t  torch.Size([128, 6, 28, 28])\n",
    "Sigmoid output shape:\t  torch.Size([128, 6, 28, 28])\n",
    "BatchNorm2d output shape: torch.Size([128, 6, 28, 28])\n",
    "AvgPool2d output shape:\t  torch.Size([128, 6, 14, 14])\n",
    "Conv2d output shape:\t  torch.Size([128, 16, 10, 10])\n",
    "Sigmoid output shape:\t  torch.Size([128, 16, 10, 10])\n",
    "BatchNorm2d output shape: torch.Size([128, 16, 10, 10])\n",
    "AvgPool2d output shape:\t  torch.Size([128, 16, 5, 5])\n",
    "Flatten output shape:\t  torch.Size([128, 400])\n",
    "Linear output shape:\t  torch.Size([128, 120])\n",
    "Sigmoid output shape:\t  torch.Size([128, 120])\n",
    "BatchNorm1d output shape: torch.Size([128, 120])\n",
    "Linear output shape:\t  torch.Size([128, 84])\n",
    "Sigmoid output shape:\t  torch.Size([128, 84])\n",
    "BatchNorm1d output shape: torch.Size([128, 84])\n",
    "Linear output shape:\t  torch.Size([128, 10])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T12:20:53.114735Z",
     "start_time": "2024-03-13T12:20:53.094243Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LeNetCNNModelBatchNorm(d2l.Classifier):\n",
    "    def __init__(self, lr=0.1, weight_decay=1e-4, momentum=0.4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # TODO add BatchNorm Layers\n",
    "        # self.net = nn.Sequential(...)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(6, kernel_size=5, padding=2), nn.Sigmoid(), nn.LazyBatchNorm2d(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.LazyConv2d(16, kernel_size=5), nn.Sigmoid(), nn.LazyBatchNorm2d(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(120), nn.Sigmoid(), nn.LazyBatchNorm1d(),\n",
    "            nn.LazyLinear(84), nn.Sigmoid(), nn.LazyBatchNorm1d(),\n",
    "            nn.LazyLinear(10))\n",
    "    def training_step(self, batch):\n",
    "        Y_hat = self(*batch[:-1])\n",
    "        loss = self.loss(Y_hat, batch[-1])\n",
    "        self.plot('loss', loss, train=True)\n",
    "        self.plot('acc', self.accuracy(Y_hat, batch[-1]), train=True)\n",
    "        return loss # the package takes care of the\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        Y_hat = self(*batch[:-1])\n",
    "        self.plot('loss', self.loss(Y_hat, batch[-1]), train=False)\n",
    "        self.plot('acc', self.accuracy(Y_hat, batch[-1]), train=False)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=self.lr, \n",
    "                               weight_decay=self.weight_decay)\n",
    "        return optimizer\n",
    "    \n",
    "model = LeNetCNNModelBatchNorm(lr=1)\n",
    "model.layer_summary(X_shape=X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "let's train the model and observe the training and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LeNetCNNModelBatchNorm(lr=1)\n",
    "model.apply_init([X], init_cnn)\n",
    "\n",
    "trainer = d2l.Trainer(max_epochs=5, num_gpus=1)\n",
    "trainer.fit(model, fashionMNIST)\n",
    "\n",
    "validate(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Run 4 Model - Dropout\n",
    "\n",
    "Let's now also add dropout:\n",
    "\n",
    "**Task**\n",
    "* add `nn.Dropout2d()` and `nn.Dropout1d()` layers in the model\n",
    "\n",
    "The layer summary should look like this:\n",
    "```\n",
    "Conv2d output shape:\t   torch.Size([128, 6, 28, 28])\n",
    "Dropout2d output shape:\t   torch.Size([128, 6, 28, 28])\n",
    "Sigmoid output shape:\t   torch.Size([128, 6, 28, 28])\n",
    "BatchNorm2d output shape:  torch.Size([128, 6, 28, 28])\n",
    "AvgPool2d output shape:\t   torch.Size([128, 6, 14, 14])\n",
    "Conv2d output shape:\t   torch.Size([128, 16, 10, 10])\n",
    "Dropout2d output shape:\t   torch.Size([128, 16, 10, 10])\n",
    "Sigmoid output shape:\t   torch.Size([128, 16, 10, 10])\n",
    "BatchNorm2d output shape:  torch.Size([128, 16, 10, 10])\n",
    "AvgPool2d output shape:\t   torch.Size([128, 16, 5, 5])\n",
    "Flatten output shape:\t   torch.Size([128, 400])\n",
    "Linear output shape:\t   torch.Size([128, 120])\n",
    "Dropout1d output shape:\t   torch.Size([128, 120])\n",
    "Sigmoid output shape:\t   torch.Size([128, 120])\n",
    "BatchNorm1d output shape:  torch.Size([128, 120])\n",
    "Linear output shape:\t   torch.Size([128, 84])\n",
    "Dropout1d output shape:\t   torch.Size([128, 84])\n",
    "Sigmoid output shape:\t   torch.Size([128, 84])\n",
    "BatchNorm1d output shape:  torch.Size([128, 84])\n",
    "Linear output shape:\t   torch.Size([128, 10])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T12:26:40.545195Z",
     "start_time": "2024-03-13T12:26:40.522565Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LeNetCNNModelBatchNormDropout(d2l.Classifier):\n",
    "    def __init__(self, lr=0.1, weight_decay=1e-4, momentum=0.4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        # TODO add nn.Dropout2d() and nn.Dropout1d() to the model from above\n",
    "        # self.net = nn.Sequential(...)\n",
    "        \n",
    "        \n",
    "    def training_step(self, batch):\n",
    "        Y_hat = self(*batch[:-1])\n",
    "        loss = self.loss(Y_hat, batch[-1])\n",
    "        self.plot('loss', loss, train=True)\n",
    "        self.plot('acc', self.accuracy(Y_hat, batch[-1]), train=True)\n",
    "        return loss # the package takes care of the\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        Y_hat = self(*batch[:-1])\n",
    "        self.plot('loss', self.loss(Y_hat, batch[-1]), train=False)\n",
    "        self.plot('acc', self.accuracy(Y_hat, batch[-1]), train=False)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=self.lr, \n",
    "                               weight_decay=self.weight_decay)\n",
    "        return optimizer\n",
    "    \n",
    "model = LeNetCNNModelBatchNormDropout(lr=1, weight_decay=1e-4, momentum=0.5)\n",
    "model.layer_summary(X_shape=X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LeNetCNNModelBatchNormDropout(lr=1, weight_decay=1e-4, momentum=0.5)\n",
    "model.apply_init([X], init_cnn)\n",
    "trainer = d2l.Trainer(max_epochs=5, num_gpus=1)\n",
    "trainer.fit(model, fashionMNIST)\n",
    "\n",
    "validate(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Questions\n",
    "\n",
    "1. Can you explain why the initialization of neural network (Run 1 vs Run 2) matters with loss surfaces?\n",
    "\n",
    "\n",
    "\n",
    "2. Why does Dropout (Run 4) and BatchNorm (Run 3) increase the training loss compared to the validation loss? What is different in BatchNorm and Dropout during training compared to validation/testing?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
