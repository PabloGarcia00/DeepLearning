{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_arhPUkn4yYA"
   },
   "source": [
    "# Regularizations with CNNs\n",
    "\n",
    "## Lab 1 Regularization by Weight Decay & Momentum\n",
    "\n",
    "Author: M. Ru√üwurm, 2024, based on notebooks from D.Tuia (2020)\n",
    "\n",
    "In this lab, we first explore the effect weight decay on training and model parameter. Then we improve training by adding a momentum term to the optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Setup\n",
    "\n",
    "Let's get the required python packages\n",
    "\n",
    "**d2l** Package:\n",
    "The \"d2l\" (short for \"dive into deep learning\") package is a Python library designed to accompany the book \"Dive into Deep Learning\"\n",
    "\n",
    "**Pytoch**:\n",
    "Pytorch is an open-source machine learning library and scientific computing framework, primarily used for deep learning applications. \n",
    "\n",
    "**sklearn.metrics**:\n",
    "The \"sklearn.metrics\" module is part of the scikit-learn library, a popular machine learning library in Python. The metrics module specifically focuses on providing tools for evaluating the performance of machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T15:21:10.218157Z",
     "start_time": "2024-03-12T15:21:06.413021Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install -q d2l\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "TqGqJ5fk1f3U"
   },
   "source": [
    "## Data - FashionMNIST\n",
    "\n",
    "Let's start by loading FashionMNIST data\n",
    "\n",
    "Fashion MNIST is a dataset used in machine learning and computer vision, serving as a benchmark for image classification tasks. It consists of 70,000 grayscale images of clothing items, categorized into 10 classes such as t-shirts, dresses, and sneakers. Fashion MNIST is a popular alternative to the traditional handwritten digit MNIST dataset, providing a more complex challenge for developing and testing image recognition algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T15:21:12.268119Z",
     "start_time": "2024-03-12T15:21:10.219359Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "q0_l88B-bawo",
    "outputId": "f374a494-f053-4429-81c7-60414870a196"
   },
   "outputs": [],
   "source": [
    "fashionMNIST = d2l.FashionMNIST(batch_size=128)\n",
    "\n",
    "train_dataloader = fashionMNIST.get_dataloader(train=True)\n",
    "val_dataloader = fashionMNIST.get_dataloader(train=False)\n",
    "\n",
    "text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    X,y = batch\n",
    "    fashionMNIST.visualize(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T15:24:26.568376Z",
     "start_time": "2024-03-12T15:24:26.557488Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(model, dataloader):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for X,y in dataloader:\n",
    "        y_true.append(y)\n",
    "        y_pred.append(model(X).argmax(1))\n",
    "        \n",
    "    y_true = torch.hstack(y_true)\n",
    "    y_pred = torch.hstack(y_pred)\n",
    "    \n",
    "    print(classification_report(y_pred=y_pred.numpy(), y_true=y_true.numpy(), labels=torch.arange(10).numpy(), target_names=text_labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOMGikUM4QaY"
   },
   "source": [
    "## Model - Multi-Layer Perceptron\n",
    "\n",
    "Let's create an 3-layer MLP for this lab\n",
    "\n",
    "**Task**\n",
    "* add a `lr` and `weight` to the `__init__` function and store their values in the instance using, for instance `self.lr = ..`. This allows us to modify learning rate and weight decay later without de-defining the entire `MLPModel(d2l.Classifier)` class\n",
    "* add the `torch.optim.SGD` optimizer to the `configure_optimizers(self)` member function of the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T15:24:27.167288Z",
     "start_time": "2024-03-12T15:24:27.165232Z"
    },
    "id": "b87FR4_m1f3V"
   },
   "outputs": [],
   "source": [
    "class MLPModel(d2l.Classifier):\n",
    "    \n",
    "    # TODO: add lr and weight decay parameters to the init\n",
    "    #def __init__(self, num_classes=10, ...):\n",
    "    #    super().__init__()\n",
    "    #    \n",
    "    #    self.lr = ...\n",
    "    #    self.weight_decay = ... \n",
    "    \n",
    "    \n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(120), nn.Sigmoid(),\n",
    "            nn.LazyLinear(84), nn.Sigmoid(),\n",
    "            nn.LazyLinear(num_classes))\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        Y_hat = self(*batch[:-1])\n",
    "        loss = self.loss(Y_hat, batch[-1])\n",
    "        self.plot('loss', loss, train=True)\n",
    "        self.plot('acc', self.accuracy(Y_hat, batch[-1]), train=True)\n",
    "        return loss # the package takes care of the\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        Y_hat = self(*batch[:-1])\n",
    "        self.plot('loss', self.loss(Y_hat, batch[-1]), train=False)\n",
    "        self.plot('acc', self.accuracy(Y_hat, batch[-1]), train=False)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        # TODO: implement the torch.optim.SGD optimizer with the self.lr and self.weight_decay parameters that are passed from the init function\n",
    "        #optimizer = ...\n",
    "        \n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model Run #1 - No Weight Decay\n",
    "\n",
    "**Tasks**:\n",
    "* initialize the model defined above with a 1 learning rate and 0 weight decay\n",
    "* fit the model to the trainer using the `trainer.fit()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BLIxJSVE1f3V",
    "outputId": "ce5dda6a-35a4-4e71-c211-393183fe8608"
   },
   "outputs": [],
   "source": [
    "# TODO: initialize an instance of the model with the learning rate and weight decay specified above\n",
    "# model = ...\n",
    "\n",
    "\n",
    "model.layer_summary(X_shape=X.shape)\n",
    "\n",
    "trainer = d2l.Trainer(max_epochs=5, num_gpus=1)\n",
    "\n",
    "# TODO: Fit the model\n",
    "# trainer.fit(...)\n",
    "\n",
    "\n",
    "validate(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The following code \n",
    "* iterates throgh all parameters, reshapes them to a vector, and stacks all weight vectors to one `w`. \n",
    "* calculates the L2-Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T08:56:37.452178Z",
     "start_time": "2024-03-13T08:56:37.432753Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = torch.hstack([param.view(-1) for param in model.parameters() if param.requires_grad])\n",
    "l2_norm = torch.norm(w, p=2)\n",
    "print(\"L2 Norm of all parameters: {:.4f}\".format(l2_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model Run #2 - Weight decay 0.0001 (or 1e-4)\n",
    "\n",
    "**Tasks**\n",
    "* re-train the model from above with the weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T15:21:36.852396Z",
     "start_time": "2024-03-12T15:21:36.852260Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO retrain the model from above\n",
    "# ...\n",
    "\n",
    "\n",
    "\n",
    "w = torch.hstack([param.view(-1) for param in model.parameters() if param.requires_grad])\n",
    "l2_norm = torch.norm(w, p=2)\n",
    "print(\"L2 Norm of all parameters: {:.4f}\".format(l2_norm))\n",
    "\n",
    "validate(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model Run #3 - Weight decay 0.01 (or 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-12T15:21:36.854370Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO retrain the model with the weight decay above\n",
    "# ...\n",
    "\n",
    "\n",
    "\n",
    "w = torch.hstack([param.view(-1) for param in model.parameters() if param.requires_grad])\n",
    "l2_norm = torch.norm(w, p=2)\n",
    "print(\"L2 Norm of all parameters: {:.4f}\".format(l2_norm))\n",
    "\n",
    "validate(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model Run #4 - Weight decay 0.0001 & Momentum 0.5\n",
    "\n",
    "**Tasks**\n",
    "* add a momentum term to the init function as `self.momentum`\n",
    "* pass it to the optimizer in `def configure_optimizers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-12T15:21:36.856259Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MLPModel(d2l.Classifier):\n",
    "    \n",
    "    # TODO: add momentum to the init function\n",
    "    #def __init__(self, lr=0.1, weight_decay=1e-4, num_classes=10, ...):\n",
    "    #    super().__init__()\n",
    "    #    self.lr = lr\n",
    "    #    self.weight_decay = weight_decay\n",
    "    #    self.mometum = ...\n",
    "    \n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-12T15:21:36.857092Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = MLPModel(lr=1, weight_decay=1e-4, momentum=0.5)\n",
    "model.layer_summary(X_shape=X.shape)\n",
    "\n",
    "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
    "trainer.fit(model, fashionMNIST)\n",
    "\n",
    "w = torch.hstack([param.view(-1) for param in model.parameters() if param.requires_grad])\n",
    "l2_norm = torch.norm(w, p=2)\n",
    "print(\"L2 Norm of all parameters: {:.4f}\".format(l2_norm))\n",
    "\n",
    "validate(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Questions\n",
    "\n",
    "1. How does L2-regularization (i.e., weight decay) in ridge regression affect the norm of parameters of a linear model?\n",
    "\n",
    "\n",
    "\n",
    "2. How does the weight decay parameter affect the L2 norm of the trained MLP model? Why is it different here?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
