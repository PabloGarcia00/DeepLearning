{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_arhPUkn4yYA"
   },
   "source": [
    "# Regularizations with CNNs\n",
    "\n",
    "## Lab 2 Regularization by Model Size\n",
    "\n",
    "Author: M. Ru√üwurm, 2024, based on notebooks from D.Tuia (2020)\n",
    "\n",
    "In this lab, we start with a complex CNN model that does not train. We simplify it by removing complexity. \n",
    "\n",
    "Main takeaway from this lab:\n",
    "Dont Overkill: Use simple models for simple problems (like FashionMNIST) and complex models for complex problems. $\\leftarrow$ this is prior knowledge (about the problem) and inducing bias (through changing model architectures) in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Setup\n",
    "\n",
    "Let's get the required python packages\n",
    "\n",
    "**d2l** Package:\n",
    "The \"d2l\" (short for \"dive into deep learning\") package is a Python library designed to accompany the book \"Dive into Deep Learning\"\n",
    "\n",
    "**Pytoch**:\n",
    "Pytorch is an open-source machine learning library and scientific computing framework, primarily used for deep learning applications. \n",
    "\n",
    "**sklearn.metrics**:\n",
    "The \"sklearn.metrics\" module is part of the scikit-learn library, a popular machine learning library in Python. The metrics module specifically focuses on providing tools for evaluating the performance of machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:35:03.304983Z",
     "start_time": "2024-03-13T11:35:01.261339Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install -q d2l\n",
    "\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import classification_report\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "TqGqJ5fk1f3U"
   },
   "source": [
    "## Data - FashionMNIST\n",
    "\n",
    "Let's start by loading FashionMNIST data\n",
    "\n",
    "Fashion MNIST is a dataset used in machine learning and computer vision, serving as a benchmark for image classification tasks. It consists of 70,000 grayscale images of clothing items, categorized into 10 classes such as t-shirts, dresses, and sneakers. Fashion MNIST is a popular alternative to the traditional handwritten digit MNIST dataset, providing a more complex challenge for developing and testing image recognition algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:35:05.888030Z",
     "start_time": "2024-03-13T11:35:03.326759Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "q0_l88B-bawo",
    "outputId": "f374a494-f053-4429-81c7-60414870a196"
   },
   "outputs": [],
   "source": [
    "\n",
    "fashionMNIST = d2l.FashionMNIST(batch_size=512)\n",
    "\n",
    "train_dataloader = fashionMNIST.get_dataloader(train=True)\n",
    "val_dataloader = fashionMNIST.get_dataloader(train=False)\n",
    "\n",
    "text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    X,y = batch\n",
    "    fashionMNIST.visualize(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Later, we would like to validate the model. \n",
    "This given function \n",
    "1. iterates through all data in a (validaiton) dataloader\n",
    "2. stores the ground truth (y_true) and predictions (y_pred)\n",
    "3. prints a classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:35:05.963128Z",
     "start_time": "2024-03-13T11:35:05.902182Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(model, dataloader):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for X,y in dataloader:\n",
    "        y_true.append(y)\n",
    "        y_pred.append(model(X).argmax(1))\n",
    "        \n",
    "    y_true = torch.hstack(y_true)\n",
    "    y_pred = torch.hstack(y_pred)\n",
    "    \n",
    "    print(classification_report(y_pred=y_pred.numpy(), y_true=y_true.numpy(), labels=torch.arange(10).numpy(), target_names=text_labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOMGikUM4QaY"
   },
   "source": [
    "## Run 1: Model - Convolutional Neural Network (LeNet)\n",
    "\n",
    "Let's create an instance of the LeNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:41:40.608552Z",
     "start_time": "2024-03-13T11:41:40.588801Z"
    },
    "id": "b87FR4_m1f3V"
   },
   "outputs": [],
   "source": [
    "class LeNetModel(d2l.Classifier):\n",
    "    def __init__(self, num_classes=10, lr=1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(6, kernel_size=5, padding=2), nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.LazyConv2d(16, kernel_size=5), nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(120), nn.Sigmoid(),\n",
    "            nn.LazyLinear(84), nn.Sigmoid(),\n",
    "            nn.LazyLinear(num_classes))\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        Y_hat = self(*batch[:-1])\n",
    "        loss = self.loss(Y_hat, batch[-1])\n",
    "        self.plot('loss', loss, train=True)\n",
    "        self.plot('acc', self.accuracy(Y_hat, batch[-1]), train=True)\n",
    "        return loss # the package takes care of the\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        Y_hat = self(*batch[:-1])\n",
    "        self.plot('loss', self.loss(Y_hat, batch[-1]), train=False)\n",
    "        self.plot('acc', self.accuracy(Y_hat, batch[-1]), train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:41:41.150379Z",
     "start_time": "2024-03-13T11:41:41.125008Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BLIxJSVE1f3V",
    "outputId": "ce5dda6a-35a4-4e71-c211-393183fe8608"
   },
   "outputs": [],
   "source": [
    "model = LeNetModel()\n",
    "model.layer_summary(X_shape=X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Task**\n",
    "* initialize the LeNetModel with a learning rate of 1 and train it on the data fashionMNIST for 5 epochs\n",
    "\n",
    "Hint: dont get stuck when the model does not train well. This is by design. Sometimes larger models are hard to train and dont find a good solution in the first place. Go ahead and simplify the model in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "zQSEPleL1f3V",
    "outputId": "0937cf53-7330-40b5-ed24-9e5f217c56f2"
   },
   "outputs": [],
   "source": [
    "# TODO train the model \n",
    "# model = ...\n",
    "# trainer = ...\n",
    "# trainer.fit(...)\n",
    "\n",
    "#SOLUTIONSTART\n",
    "model = LeNetModel()\n",
    "\n",
    "trainer = d2l.Trainer(max_epochs=5, num_gpus=1) # set num_gps\n",
    "trainer.fit(model, fashionMNIST)\n",
    "#SOLUTIONEND\n",
    "\n",
    "validate(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "hM6N4_i51f3V"
   },
   "source": [
    "## Run 2 - MLP Model - remove the convolutions from the LeNet.\n",
    "\n",
    "Removing complexity is a first step for regularization by simplifying the model architecture. It makes models run faster and can yield surprisingly good results.\n",
    "No Overkill: Use simple models for simple problems (like FashionMNIST) and complex models for complex problems. $\\leftarrow$ this is prior knowledge and inductive bias in practice.\n",
    "\n",
    "**Task**\n",
    "* Simplify the model by removing (e.g., commenting out the Convolution and pooling layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 569
    },
    "id": "CnuGP8zH1f3W",
    "outputId": "178e664d-6b71-4904-f6a3-03ea9d55e1cf"
   },
   "outputs": [],
   "source": [
    "class MLPModel(d2l.Classifier):\n",
    "    def __init__(self, num_classes=10, lr=1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        #TODO: copy the self.net from above but remove the CNN layer\n",
    "        #self.net = nn.Sequential(...)\n",
    "        #SOLUTIONSTART\n",
    "        self.net = nn.Sequential(\n",
    "            #nn.LazyConv2d(6, kernel_size=5),\n",
    "            #nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            #nn.LazyConv2d(16, kernel_size=5),\n",
    "            #nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(), nn.LazyLinear(120),\n",
    "            nn.Sigmoid(), nn.LazyLinear(84),\n",
    "            nn.Sigmoid(), nn.LazyLinear(num_classes))\n",
    "        #SOLUTIONEND\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        Y_hat = self(*batch[:-1])\n",
    "        loss = self.loss(Y_hat, batch[-1])\n",
    "        self.plot('loss', loss, train=True)\n",
    "        self.plot('acc', self.accuracy(Y_hat, batch[-1]), train=True)\n",
    "        return loss # the package takes care of the\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        Y_hat = self(*batch[:-1])\n",
    "        self.plot('loss', self.loss(Y_hat, batch[-1]), train=False)\n",
    "        self.plot('acc', self.accuracy(Y_hat, batch[-1]), train=False)\n",
    "\n",
    "model = MLPModel()\n",
    "model.layer_summary(X_shape=X.shape)\n",
    "\n",
    "trainer = d2l.Trainer(max_epochs=10)\n",
    "trainer.fit(model, fashionMNIST)\n",
    "validate(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "xAavqJFm1f3W"
   },
   "source": [
    "## Deeper MLP Model\n",
    "\n",
    "For reference, let's complicate the model again by adding additional linear layers to make it deeper.\n",
    "\n",
    "**Task**\n",
    "* add two additional nn.LazyLinear(120) layers to the MLP model from above.\n",
    "\n",
    "the model summary should look like this:\n",
    "\n",
    "```\n",
    "Flatten output shape:\t torch.Size([512, 784])\n",
    "Linear output shape:\t torch.Size([512, 120])\n",
    "Sigmoid output shape:\t torch.Size([512, 120])\n",
    "Linear output shape:\t torch.Size([512, 120])\n",
    "Sigmoid output shape:\t torch.Size([512, 120])\n",
    "Linear output shape:\t torch.Size([512, 120])\n",
    "Sigmoid output shape:\t torch.Size([512, 120])\n",
    "Linear output shape:\t torch.Size([512, 84])\n",
    "Sigmoid output shape:\t torch.Size([512, 84])\n",
    "Linear output shape:\t torch.Size([512, 10])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:54:38.623572Z",
     "start_time": "2024-03-13T11:54:38.606524Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613
    },
    "id": "DOwI6WbQ1f3W",
    "outputId": "0b8a2cea-cd44-458f-bbee-cd6432d2dbcc"
   },
   "outputs": [],
   "source": [
    "class DeepMLPModel(d2l.Classifier):\n",
    "    def __init__(self, lr=1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        #TODO: add two additional nn.LazyLinear(120) layers to the MLP model from above.\n",
    "        #self.net = nn.Sequential(...)\n",
    "        #SOLUTIONSTART\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(120), nn.Sigmoid(),\n",
    "            nn.LazyLinear(120), nn.Sigmoid(),\n",
    "            nn.LazyLinear(120), nn.Sigmoid(),\n",
    "            nn.LazyLinear(84), nn.Sigmoid(),\n",
    "            nn.LazyLinear(num_classes))\n",
    "        #SOLUTIONEND\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        Y_hat = self(*batch[:-1])\n",
    "        loss = self.loss(Y_hat, batch[-1])\n",
    "        self.plot('loss', loss, train=True)\n",
    "        self.plot('acc', self.accuracy(Y_hat, batch[-1]), train=True)\n",
    "        return loss # the package takes care of the\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        Y_hat = self(*batch[:-1])\n",
    "        self.plot('loss', self.loss(Y_hat, batch[-1]), train=False)\n",
    "        self.plot('acc', self.accuracy(Y_hat, batch[-1]), train=False)\n",
    "\n",
    "model = DeepMLPModel()\n",
    "model.layer_summary(X_shape=X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = DeepMLPModel()\n",
    "\n",
    "trainer = d2l.Trainer(max_epochs=20)\n",
    "trainer.fit(model, fashionMNIST)\n",
    "validate(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Questions\n",
    "\n",
    "1. Why did the CNN (Run 1) not train? Please explain with a loss surface in mind.\n",
    "\n",
    "#SOLUTIONSTART\n",
    "The model was too complex. Without any regularization, the loss surface was full of local minima that the gradient descent algorithm could not escape. So a better minimum was not found\n",
    "#SOLUTIONEND\n",
    "\n",
    "2. In Run 2, you simplified the model by removing the convolutions. Why did this model train now?\n",
    "\n",
    "#SOLUTIONSTART\n",
    "Removing layers and simplifying the model removes complexity. This effectively smoothes the loss surface, which makes sure that the model finds a good optimimum from any initialization.\n",
    "#SOLUTIONEND\n",
    "\n",
    "3. In Run 3, the training curve does not look promising until it suddenly decreases. What is going on in terms of gradient descent? How do the deeper layers affect the loss surface?\n",
    "\n",
    "#SOLUTIONSTART\n",
    "Here, the model was stuck on a saddle point for a while before moving into an optimum. Similar to Q2, additional layers (depth) increase complexity and make it more difficult for the model to find an optimum.\n",
    "#SOLUTIONEND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
