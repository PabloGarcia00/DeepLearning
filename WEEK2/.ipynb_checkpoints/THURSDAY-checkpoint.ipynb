{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "763285b3-cd38-48ef-9f88-edbb76e5fd9c",
   "metadata": {},
   "source": [
    "# Friday: paper review\n",
    "\n",
    "[Deep Residual Learning for Image Recognition](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)\n",
    "\n",
    "* We present a residual learning framework for very deep networks.\n",
    "* low complexity while being very deep i.e. over 16 layers\n",
    "* higher training error is not caused by overfitting but by adding more layers (FIG1).\n",
    "* our extremely deep residual nets are easy to optimize\n",
    "* our deep residual nets can easily enjor accuracy gains from greatly increased depth, producing results subtantially better than previous networks.\n",
    "\n",
    "\n",
    "*Is learning better networks as easy as stacking more layers*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154cec6d-2e9e-449f-b740-231f665f4fde",
   "metadata": {},
   "source": [
    "my questions:\n",
    "* how is model complexity defined &rarr; number of paremeters to estimate &rarr; number of **Floating Point Operations FLOPs**\n",
    "* what is the problem of vanishing/exploding gradients and why does it hamper convergence, and how is fixed by normalized initilization.\n",
    "\n",
    "    * gradients vanish when the activation function use values ranging between 0 and 1, tuning of the parameters are so small that the model isn't learning. Vice versa exploding gradients happen when the activation function uses values higher than 1. The problem is present in very deep models where the multiplication happens every layer increasing this problem.\n",
    "  \n",
    "    * this results in slow training and poor learning.\n",
    "    * can be solved:\n",
    "        *  by using good initialization strategies: Xavier, He\n",
    "        *  Activation function: ReLU\n",
    "        *  Gradient clipping: limit the maximum value of gradients\n",
    "        *  residual networks: using skip connections\n",
    "          \n",
    "* what is stochastic gradient descent and how is it different from gradient descent\n",
    "    * (classic) gradient descent considers the whole training set to calculate the gradient, this is very computationally taxing, hence stochastic gradient descent which subset the set.\n",
    " \n",
    "* what is a degradation problem\n",
    "    * the decrease in performance with the addition of more layers.\n",
    " \n",
    "* how do you differentiate the *gradient problem* from finiding a local minima rather.\n",
    "    * during a plateau in the error margin they aplied they multiplied the learning rate by 10.\n",
    "\n",
    "* how to distinguish the *gradient* problem from overfitting\n",
    "    * by looking at the training and test error side by side and including a parallel model with less layers, and check if said model performs better.\n",
    " \n",
    "* bottleneck design\n",
    "    * using blocks e.g. 1x1, 3x3, and 1x1 kernels where 1x1 kernels essentially perform element wise operation on the data, and the first 1x1 reduces dimensions and the latter increases dimensions (to manage the input data for the next layer).\n",
    "    * the 3x3 layers is used for feature detection.\n",
    "    * this method reduces computational power necessary and the dimension reduction reduces the number of parameters that need to be tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9514a040-2f2b-467d-9265-06fa2457c648",
   "metadata": {},
   "source": [
    "The solution to *degradation* problem: by construction\n",
    "to the deeper model: the added layers are identity mapping,\n",
    "and the other layers are copied from the learned shallower\n",
    "model. The existence of this constructed solution indicates\n",
    "that a deeper model should produce no higher training error\n",
    "than its shallower counterpart\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ee33f4-dce7-459d-a14b-6cebd74e52aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
